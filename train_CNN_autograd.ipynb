{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(1000, 1, 28, 28) (1000, 10)\n",
      "(1000, 1, 28, 28) (1000, 10)\n",
      "(10000, 1, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from MLP_autograd import *\n",
    "from CNN_autograd import *\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "def one_hot(Y):\n",
    "  one_hot_Y = np.zeros((Y.size,np.max(Y) + 1))\n",
    "  one_hot_Y[np.arange(Y.size),Y] = 1\n",
    "  return one_hot_Y\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "y_train = one_hot(y_train)\n",
    "y_test = one_hot(y_test)\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "x_validation = x_train[59000:60000]\n",
    "y_validation = y_train[59000:60000,:]\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[0:1000,:]\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0],1,28,28)\n",
    "x_validation = x_validation.reshape(x_validation.shape[0],1,28,28)\n",
    "x_test = x_test.reshape(x_test.shape[0],1,28,28)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_validation.shape, y_validation.shape)\n",
    "print(x_test.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (1000, 1, 28, 28)\n",
      "y_train:  (1000, 10)\n",
      "x_val:  (1000, 1, 28, 28)\n",
      "y_val:  (1000, 10)\n",
      "epochs:  0 loss_train:  11.660397084609277 accuracy_train: 0.108 loss_val 11.2267773155034 accuracy_validation: 0.146\n",
      "epochs:  1 loss_train:  10.916986547256087 accuracy_train: 0.151 loss_val 10.627423550901467 accuracy_validation: 0.169\n",
      "epochs:  2 loss_train:  10.479233307021564 accuracy_train: 0.163 loss_val 10.331757187990522 accuracy_validation: 0.173\n",
      "epochs:  3 loss_train:  10.190034998638327 accuracy_train: 0.164 loss_val 10.116258058822352 accuracy_validation: 0.173\n",
      "epochs:  4 loss_train:  9.842698734723918 accuracy_train: 0.169 loss_val 9.953911927681078 accuracy_validation: 0.177\n",
      "epochs:  5 loss_train:  9.462934791089507 accuracy_train: 0.184 loss_val 9.737815243511552 accuracy_validation: 0.188\n",
      "epochs:  6 loss_train:  9.255678566814444 accuracy_train: 0.197 loss_val 9.527009618004614 accuracy_validation: 0.187\n",
      "epochs:  7 loss_train:  9.07257497339137 accuracy_train: 0.211 loss_val 9.323494548794619 accuracy_validation: 0.188\n",
      "epochs:  8 loss_train:  8.832348581113001 accuracy_train: 0.213 loss_val 9.083780247225505 accuracy_validation: 0.207\n",
      "epochs:  9 loss_train:  8.566210494718907 accuracy_train: 0.232 loss_val 8.858701055119854 accuracy_validation: 0.211\n",
      "epochs:  10 loss_train:  8.342877075264845 accuracy_train: 0.242 loss_val 8.688350085529237 accuracy_validation: 0.218\n",
      "epochs:  11 loss_train:  8.12719727755176 accuracy_train: 0.251 loss_val 8.528737210752812 accuracy_validation: 0.223\n",
      "epochs:  12 loss_train:  7.906158095392068 accuracy_train: 0.259 loss_val 8.366390707975432 accuracy_validation: 0.227\n",
      "epochs:  13 loss_train:  7.7009316490626505 accuracy_train: 0.266 loss_val 8.195640264992564 accuracy_validation: 0.231\n",
      "epochs:  14 loss_train:  7.513458540673162 accuracy_train: 0.27 loss_val 8.02322206776589 accuracy_validation: 0.241\n",
      "epochs:  15 loss_train:  7.339120159385211 accuracy_train: 0.274 loss_val 7.8724591048360395 accuracy_validation: 0.248\n",
      "epochs:  16 loss_train:  7.172554181153604 accuracy_train: 0.291 loss_val 7.717306866012248 accuracy_validation: 0.254\n",
      "epochs:  17 loss_train:  7.000553752802562 accuracy_train: 0.3 loss_val 7.5564402680200375 accuracy_validation: 0.265\n",
      "epochs:  18 loss_train:  6.830810965587025 accuracy_train: 0.309 loss_val 7.397262650801955 accuracy_validation: 0.276\n",
      "epochs:  19 loss_train:  6.671414745026295 accuracy_train: 0.311 loss_val 7.252467621209403 accuracy_validation: 0.277\n",
      "epochs:  20 loss_train:  6.514986926441069 accuracy_train: 0.32 loss_val 7.111929226825853 accuracy_validation: 0.283\n",
      "epochs:  21 loss_train:  6.358046970895084 accuracy_train: 0.327 loss_val 6.967786357793487 accuracy_validation: 0.289\n",
      "epochs:  22 loss_train:  6.203638964006777 accuracy_train: 0.329 loss_val 6.831100801650064 accuracy_validation: 0.296\n",
      "epochs:  23 loss_train:  6.0577482830348455 accuracy_train: 0.337 loss_val 6.698320757566613 accuracy_validation: 0.305\n",
      "epochs:  24 loss_train:  5.916113492028629 accuracy_train: 0.353 loss_val 6.5739616039778666 accuracy_validation: 0.313\n",
      "epochs:  25 loss_train:  5.785310970669596 accuracy_train: 0.356 loss_val 6.459871310128768 accuracy_validation: 0.324\n",
      "epochs:  26 loss_train:  5.654986254790693 accuracy_train: 0.37 loss_val 6.348315777167347 accuracy_validation: 0.323\n",
      "epochs:  27 loss_train:  5.532055467992123 accuracy_train: 0.374 loss_val 6.240650295451887 accuracy_validation: 0.33\n",
      "epochs:  28 loss_train:  5.416900707663166 accuracy_train: 0.376 loss_val 6.139238181688724 accuracy_validation: 0.341\n",
      "epochs:  29 loss_train:  5.307402934648611 accuracy_train: 0.385 loss_val 6.047787016462358 accuracy_validation: 0.354\n",
      "epochs:  30 loss_train:  5.19791523465281 accuracy_train: 0.392 loss_val 5.950062737865404 accuracy_validation: 0.36\n",
      "epochs:  31 loss_train:  5.098027324994001 accuracy_train: 0.399 loss_val 5.866792440423323 accuracy_validation: 0.37\n",
      "epochs:  32 loss_train:  4.99974109163135 accuracy_train: 0.407 loss_val 5.77998103194816 accuracy_validation: 0.375\n",
      "epochs:  33 loss_train:  4.905829050651289 accuracy_train: 0.412 loss_val 5.701215249884552 accuracy_validation: 0.378\n",
      "epochs:  34 loss_train:  4.8151880780056855 accuracy_train: 0.421 loss_val 5.625230787068372 accuracy_validation: 0.382\n",
      "epochs:  35 loss_train:  4.729926285768995 accuracy_train: 0.419 loss_val 5.549116255698274 accuracy_validation: 0.386\n",
      "epochs:  36 loss_train:  4.647540384839308 accuracy_train: 0.423 loss_val 5.475323815569413 accuracy_validation: 0.391\n",
      "epochs:  37 loss_train:  4.565467055960769 accuracy_train: 0.428 loss_val 5.403997300805791 accuracy_validation: 0.396\n",
      "epochs:  38 loss_train:  4.489648288274118 accuracy_train: 0.428 loss_val 5.338809291621503 accuracy_validation: 0.398\n",
      "epochs:  39 loss_train:  4.415286536592073 accuracy_train: 0.434 loss_val 5.274271574027677 accuracy_validation: 0.398\n",
      "epochs:  40 loss_train:  4.339929125590761 accuracy_train: 0.435 loss_val 5.207911745069757 accuracy_validation: 0.401\n",
      "epochs:  41 loss_train:  4.266494934210885 accuracy_train: 0.437 loss_val 5.139814278045268 accuracy_validation: 0.404\n",
      "epochs:  42 loss_train:  4.192653627083537 accuracy_train: 0.438 loss_val 5.073761005530005 accuracy_validation: 0.409\n",
      "epochs:  43 loss_train:  4.1227175815730535 accuracy_train: 0.439 loss_val 5.014000072804694 accuracy_validation: 0.413\n",
      "epochs:  44 loss_train:  4.053350391091621 accuracy_train: 0.442 loss_val 4.95227470624352 accuracy_validation: 0.418\n",
      "epochs:  45 loss_train:  3.9895157173826683 accuracy_train: 0.446 loss_val 4.895628127016519 accuracy_validation: 0.424\n",
      "epochs:  46 loss_train:  3.9253967099645566 accuracy_train: 0.449 loss_val 4.839182636256019 accuracy_validation: 0.425\n",
      "epochs:  47 loss_train:  3.8657835583602203 accuracy_train: 0.457 loss_val 4.790292616355216 accuracy_validation: 0.425\n",
      "epochs:  48 loss_train:  3.80825509775297 accuracy_train: 0.467 loss_val 4.7411438164960895 accuracy_validation: 0.426\n",
      "epochs:  49 loss_train:  3.75317154790725 accuracy_train: 0.471 loss_val 4.692944924244991 accuracy_validation: 0.429\n",
      "epochs:  50 loss_train:  3.7026260509539624 accuracy_train: 0.474 loss_val 4.64749401062011 accuracy_validation: 0.431\n",
      "epochs:  51 loss_train:  3.6536558937194576 accuracy_train: 0.477 loss_val 4.604771579526679 accuracy_validation: 0.432\n",
      "epochs:  52 loss_train:  3.6039148683176916 accuracy_train: 0.479 loss_val 4.560815911753397 accuracy_validation: 0.437\n",
      "epochs:  53 loss_train:  3.5563247879784234 accuracy_train: 0.481 loss_val 4.517045974571618 accuracy_validation: 0.44\n",
      "epochs:  54 loss_train:  3.5078288747581268 accuracy_train: 0.482 loss_val 4.471241344539471 accuracy_validation: 0.445\n",
      "epochs:  55 loss_train:  3.460541494905039 accuracy_train: 0.484 loss_val 4.423999280951015 accuracy_validation: 0.443\n",
      "epochs:  56 loss_train:  3.4154029873404332 accuracy_train: 0.487 loss_val 4.377120925417835 accuracy_validation: 0.447\n",
      "epochs:  57 loss_train:  3.3728325557192385 accuracy_train: 0.492 loss_val 4.337068748899284 accuracy_validation: 0.448\n",
      "epochs:  58 loss_train:  3.329921768078519 accuracy_train: 0.497 loss_val 4.295445415662925 accuracy_validation: 0.449\n",
      "epochs:  59 loss_train:  3.288964088486793 accuracy_train: 0.503 loss_val 4.255217087957805 accuracy_validation: 0.451\n",
      "epochs:  60 loss_train:  3.2473963377640715 accuracy_train: 0.506 loss_val 4.213390231177579 accuracy_validation: 0.453\n",
      "epochs:  61 loss_train:  3.2068396769710525 accuracy_train: 0.509 loss_val 4.171673801547871 accuracy_validation: 0.455\n",
      "epochs:  62 loss_train:  3.169250947945361 accuracy_train: 0.516 loss_val 4.1349026561172995 accuracy_validation: 0.455\n",
      "epochs:  63 loss_train:  3.1307536427559772 accuracy_train: 0.521 loss_val 4.094974882098881 accuracy_validation: 0.457\n",
      "epochs:  64 loss_train:  3.0912103957081007 accuracy_train: 0.522 loss_val 4.051797163249254 accuracy_validation: 0.46\n",
      "epochs:  65 loss_train:  3.0545050625526757 accuracy_train: 0.531 loss_val 4.012459368104887 accuracy_validation: 0.466\n",
      "epochs:  66 loss_train:  3.019838604652584 accuracy_train: 0.533 loss_val 3.9750406262043643 accuracy_validation: 0.469\n",
      "epochs:  67 loss_train:  2.9838070149483427 accuracy_train: 0.541 loss_val 3.9374804556287497 accuracy_validation: 0.472\n",
      "epochs:  68 loss_train:  2.949820961683236 accuracy_train: 0.545 loss_val 3.9004665703294514 accuracy_validation: 0.473\n",
      "epochs:  69 loss_train:  2.9179899100971807 accuracy_train: 0.552 loss_val 3.8657032685977115 accuracy_validation: 0.477\n",
      "epochs:  70 loss_train:  2.885782857784072 accuracy_train: 0.553 loss_val 3.8309172887812966 accuracy_validation: 0.477\n",
      "epochs:  71 loss_train:  2.8536559487057422 accuracy_train: 0.551 loss_val 3.7952343567842237 accuracy_validation: 0.48\n",
      "epochs:  72 loss_train:  2.823450509166933 accuracy_train: 0.555 loss_val 3.766173813909448 accuracy_validation: 0.483\n",
      "epochs:  73 loss_train:  2.7944716939032364 accuracy_train: 0.556 loss_val 3.736216667105666 accuracy_validation: 0.484\n",
      "epochs:  74 loss_train:  2.7654811123919507 accuracy_train: 0.558 loss_val 3.706358420399761 accuracy_validation: 0.486\n",
      "epochs:  75 loss_train:  2.734296747121282 accuracy_train: 0.559 loss_val 3.674354782016399 accuracy_validation: 0.487\n",
      "epochs:  76 loss_train:  2.707203918151328 accuracy_train: 0.562 loss_val 3.6449880696860575 accuracy_validation: 0.487\n",
      "epochs:  77 loss_train:  2.6798876096210877 accuracy_train: 0.564 loss_val 3.61668687249136 accuracy_validation: 0.489\n",
      "epochs:  78 loss_train:  2.652042189399204 accuracy_train: 0.566 loss_val 3.58702726857901 accuracy_validation: 0.493\n",
      "epochs:  79 loss_train:  2.626078085287572 accuracy_train: 0.567 loss_val 3.558670659185145 accuracy_validation: 0.494\n",
      "epochs:  80 loss_train:  2.599578991428213 accuracy_train: 0.569 loss_val 3.5288376737484626 accuracy_validation: 0.497\n",
      "epochs:  81 loss_train:  2.5731610339647104 accuracy_train: 0.569 loss_val 3.5023879056983125 accuracy_validation: 0.501\n",
      "epochs:  82 loss_train:  2.5493257196307706 accuracy_train: 0.57 loss_val 3.4774399761382484 accuracy_validation: 0.501\n",
      "epochs:  83 loss_train:  2.5210210912112028 accuracy_train: 0.573 loss_val 3.4485398188683316 accuracy_validation: 0.502\n",
      "epochs:  84 loss_train:  2.4959551242793254 accuracy_train: 0.575 loss_val 3.4213370382246495 accuracy_validation: 0.502\n",
      "epochs:  85 loss_train:  2.4728421244378707 accuracy_train: 0.577 loss_val 3.3959455484621666 accuracy_validation: 0.502\n",
      "epochs:  86 loss_train:  2.4492501242756326 accuracy_train: 0.578 loss_val 3.3699140914200063 accuracy_validation: 0.506\n",
      "epochs:  87 loss_train:  2.4250607988106436 accuracy_train: 0.58 loss_val 3.344237960707513 accuracy_validation: 0.506\n",
      "epochs:  88 loss_train:  2.4028723792841284 accuracy_train: 0.581 loss_val 3.318764173561343 accuracy_validation: 0.506\n",
      "epochs:  89 loss_train:  2.3803727457967656 accuracy_train: 0.585 loss_val 3.293906486418663 accuracy_validation: 0.509\n",
      "epochs:  90 loss_train:  2.3568413069945136 accuracy_train: 0.587 loss_val 3.2681226986573093 accuracy_validation: 0.51\n",
      "epochs:  91 loss_train:  2.334276818610819 accuracy_train: 0.591 loss_val 3.2433512742863586 accuracy_validation: 0.512\n",
      "epochs:  92 loss_train:  2.3123540616171168 accuracy_train: 0.593 loss_val 3.219922780470649 accuracy_validation: 0.513\n",
      "epochs:  93 loss_train:  2.289420847276029 accuracy_train: 0.59 loss_val 3.194340617571457 accuracy_validation: 0.515\n",
      "epochs:  94 loss_train:  2.2677988562593 accuracy_train: 0.591 loss_val 3.171717137759883 accuracy_validation: 0.515\n",
      "epochs:  95 loss_train:  2.246809023223702 accuracy_train: 0.598 loss_val 3.1495871014190797 accuracy_validation: 0.514\n",
      "epochs:  96 loss_train:  2.2237164109038376 accuracy_train: 0.597 loss_val 3.1261039471231014 accuracy_validation: 0.517\n",
      "epochs:  97 loss_train:  2.204144536699795 accuracy_train: 0.599 loss_val 3.1045220503667235 accuracy_validation: 0.52\n",
      "epochs:  98 loss_train:  2.186782811487754 accuracy_train: 0.6 loss_val 3.0849964241230317 accuracy_validation: 0.523\n",
      "epochs:  99 loss_train:  2.1663421511793155 accuracy_train: 0.603 loss_val 3.060845259444803 accuracy_validation: 0.526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.428294414424434, 0.4837)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN1 = Convolutional(input_shape = (1,28,28), kernel_size = 3, output_depth = 4, l_rate = 0.01, activeFuncion = \"relu\")\n",
    "pool1 = MaxPoolingLayer(2)\n",
    "CNN2 = Convolutional(input_shape = (4,13,13), kernel_size = 3,output_depth = 8, l_rate = 0.01, activeFuncion = \"relu\")\n",
    "pool2 = MaxPoolingLayer(2)\n",
    "flatten = Flattening()\n",
    "nn = NeuralNetwork(layers_size=[288,10],activations = [\"softmax\"], lossFunction = \"crossEntropy\", l_rate = 0.01) # 8*6*6 = 288\n",
    "\n",
    "\n",
    "model = Model([CNN1,pool1,CNN2,pool2, flatten,nn])\n",
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (1000, 1, 28, 28)\n",
      "y_train:  (1000, 10)\n",
      "x_val:  (1000, 1, 28, 28)\n",
      "y_val:  (1000, 10)\n",
      "epochs:  0 loss_train:  2.1478017172479142 accuracy_train: 0.6 loss_val 3.040670716193919 accuracy_validation: 0.529\n",
      "epochs:  1 loss_train:  2.1235179360632896 accuracy_train: 0.607 loss_val 3.0153046995143775 accuracy_validation: 0.533\n",
      "epochs:  2 loss_train:  2.103614430505155 accuracy_train: 0.608 loss_val 2.993787103005355 accuracy_validation: 0.533\n",
      "epochs:  3 loss_train:  2.082173559899423 accuracy_train: 0.611 loss_val 2.9703897025893244 accuracy_validation: 0.536\n",
      "epochs:  4 loss_train:  2.062837475207918 accuracy_train: 0.611 loss_val 2.9498864043203556 accuracy_validation: 0.536\n",
      "epochs:  5 loss_train:  2.0442206104593197 accuracy_train: 0.612 loss_val 2.930642868556625 accuracy_validation: 0.537\n",
      "epochs:  6 loss_train:  2.0265471131638937 accuracy_train: 0.613 loss_val 2.9113918132409617 accuracy_validation: 0.537\n",
      "epochs:  7 loss_train:  2.010283305356826 accuracy_train: 0.616 loss_val 2.8934022800659593 accuracy_validation: 0.544\n",
      "epochs:  8 loss_train:  1.9911241333048724 accuracy_train: 0.618 loss_val 2.873682761395116 accuracy_validation: 0.551\n",
      "epochs:  9 loss_train:  1.9728538184294282 accuracy_train: 0.623 loss_val 2.8543631580980335 accuracy_validation: 0.553\n",
      "epochs:  10 loss_train:  1.9565132331271848 accuracy_train: 0.622 loss_val 2.8375708554358448 accuracy_validation: 0.553\n",
      "epochs:  11 loss_train:  1.9381703485553583 accuracy_train: 0.626 loss_val 2.8178158022079054 accuracy_validation: 0.556\n",
      "epochs:  12 loss_train:  1.9226479320882013 accuracy_train: 0.628 loss_val 2.800025384604046 accuracy_validation: 0.559\n",
      "epochs:  13 loss_train:  1.906372341501335 accuracy_train: 0.629 loss_val 2.7826780677875633 accuracy_validation: 0.56\n",
      "epochs:  14 loss_train:  1.891827923152493 accuracy_train: 0.633 loss_val 2.76598759138565 accuracy_validation: 0.561\n",
      "epochs:  15 loss_train:  1.8770966040339607 accuracy_train: 0.631 loss_val 2.7504672349586476 accuracy_validation: 0.563\n",
      "epochs:  16 loss_train:  1.8612654407307578 accuracy_train: 0.632 loss_val 2.734220604015046 accuracy_validation: 0.563\n",
      "epochs:  17 loss_train:  1.8457532490794093 accuracy_train: 0.634 loss_val 2.7175953527096866 accuracy_validation: 0.564\n",
      "epochs:  18 loss_train:  1.8298381122217608 accuracy_train: 0.635 loss_val 2.6993825708158106 accuracy_validation: 0.564\n",
      "epochs:  19 loss_train:  1.814496092187854 accuracy_train: 0.638 loss_val 2.6836005466614195 accuracy_validation: 0.564\n",
      "epochs:  20 loss_train:  1.7992541165265643 accuracy_train: 0.641 loss_val 2.667782748341484 accuracy_validation: 0.567\n",
      "epochs:  21 loss_train:  1.7851400298417326 accuracy_train: 0.642 loss_val 2.652533284261621 accuracy_validation: 0.569\n",
      "epochs:  22 loss_train:  1.7697710893116814 accuracy_train: 0.644 loss_val 2.6361598550644567 accuracy_validation: 0.572\n",
      "epochs:  23 loss_train:  1.754903053931019 accuracy_train: 0.644 loss_val 2.620521089574288 accuracy_validation: 0.573\n",
      "epochs:  24 loss_train:  1.741232903881061 accuracy_train: 0.647 loss_val 2.6049685512253764 accuracy_validation: 0.575\n",
      "epochs:  25 loss_train:  1.727492679058075 accuracy_train: 0.645 loss_val 2.589612045835612 accuracy_validation: 0.575\n",
      "epochs:  26 loss_train:  1.7136091196323642 accuracy_train: 0.648 loss_val 2.5740099730467145 accuracy_validation: 0.576\n",
      "epochs:  27 loss_train:  1.6990728307742862 accuracy_train: 0.649 loss_val 2.5590198951580994 accuracy_validation: 0.577\n",
      "epochs:  28 loss_train:  1.686098069854734 accuracy_train: 0.649 loss_val 2.5451777367904964 accuracy_validation: 0.577\n",
      "epochs:  29 loss_train:  1.6730911968147588 accuracy_train: 0.651 loss_val 2.531132159832281 accuracy_validation: 0.579\n",
      "epochs:  30 loss_train:  1.6604291868360184 accuracy_train: 0.652 loss_val 2.5172757669479324 accuracy_validation: 0.58\n",
      "epochs:  31 loss_train:  1.6477635351701274 accuracy_train: 0.654 loss_val 2.503360771244938 accuracy_validation: 0.58\n",
      "epochs:  32 loss_train:  1.6351426141353886 accuracy_train: 0.655 loss_val 2.489462446707555 accuracy_validation: 0.579\n",
      "epochs:  33 loss_train:  1.623959924992806 accuracy_train: 0.656 loss_val 2.476589542864465 accuracy_validation: 0.579\n",
      "epochs:  34 loss_train:  1.6122166777121563 accuracy_train: 0.659 loss_val 2.464164432869181 accuracy_validation: 0.58\n",
      "epochs:  35 loss_train:  1.6001593086276824 accuracy_train: 0.662 loss_val 2.450928505448242 accuracy_validation: 0.579\n",
      "epochs:  36 loss_train:  1.5901157472716576 accuracy_train: 0.66 loss_val 2.4384189540890313 accuracy_validation: 0.579\n",
      "epochs:  37 loss_train:  1.5783921724151817 accuracy_train: 0.659 loss_val 2.4250418086282672 accuracy_validation: 0.578\n",
      "epochs:  38 loss_train:  1.567272035421484 accuracy_train: 0.664 loss_val 2.4124440174566235 accuracy_validation: 0.578\n",
      "epochs:  39 loss_train:  1.5561532951975623 accuracy_train: 0.666 loss_val 2.3991938574387413 accuracy_validation: 0.578\n",
      "epochs:  40 loss_train:  1.54527182475025 accuracy_train: 0.669 loss_val 2.3867655312647442 accuracy_validation: 0.578\n",
      "epochs:  41 loss_train:  1.5350641257825877 accuracy_train: 0.673 loss_val 2.374630104208101 accuracy_validation: 0.582\n",
      "epochs:  42 loss_train:  1.5239849807954142 accuracy_train: 0.676 loss_val 2.3626712507017618 accuracy_validation: 0.582\n",
      "epochs:  43 loss_train:  1.5140586711806094 accuracy_train: 0.679 loss_val 2.351536995684049 accuracy_validation: 0.584\n",
      "epochs:  44 loss_train:  1.5038378905863232 accuracy_train: 0.68 loss_val 2.3399401833182005 accuracy_validation: 0.585\n",
      "epochs:  45 loss_train:  1.4964037281812739 accuracy_train: 0.68 loss_val 2.3308029113885347 accuracy_validation: 0.585\n",
      "epochs:  46 loss_train:  1.485604036017957 accuracy_train: 0.681 loss_val 2.3188785084896235 accuracy_validation: 0.585\n",
      "epochs:  47 loss_train:  1.476541169818499 accuracy_train: 0.681 loss_val 2.3084911563990165 accuracy_validation: 0.585\n",
      "epochs:  48 loss_train:  1.4670398163258302 accuracy_train: 0.682 loss_val 2.297565176859128 accuracy_validation: 0.588\n",
      "epochs:  49 loss_train:  1.4587851437398638 accuracy_train: 0.683 loss_val 2.287336116950376 accuracy_validation: 0.588\n",
      "epochs:  50 loss_train:  1.4499153355225127 accuracy_train: 0.684 loss_val 2.2775676191519634 accuracy_validation: 0.589\n",
      "epochs:  51 loss_train:  1.4406230188353244 accuracy_train: 0.686 loss_val 2.2667870600095013 accuracy_validation: 0.59\n",
      "epochs:  52 loss_train:  1.431982533049683 accuracy_train: 0.687 loss_val 2.256962352771717 accuracy_validation: 0.591\n",
      "epochs:  53 loss_train:  1.4227790115262722 accuracy_train: 0.691 loss_val 2.246543555126251 accuracy_validation: 0.592\n",
      "epochs:  54 loss_train:  1.4138506346967223 accuracy_train: 0.692 loss_val 2.236860307762905 accuracy_validation: 0.595\n",
      "epochs:  55 loss_train:  1.4050847841079077 accuracy_train: 0.692 loss_val 2.226960344635074 accuracy_validation: 0.593\n",
      "epochs:  56 loss_train:  1.3963402967325884 accuracy_train: 0.693 loss_val 2.217206294729847 accuracy_validation: 0.592\n",
      "epochs:  57 loss_train:  1.3880292120173359 accuracy_train: 0.694 loss_val 2.2074090763338345 accuracy_validation: 0.595\n",
      "epochs:  58 loss_train:  1.3804484376703008 accuracy_train: 0.694 loss_val 2.1979659717627835 accuracy_validation: 0.596\n",
      "epochs:  59 loss_train:  1.373086013509675 accuracy_train: 0.695 loss_val 2.1891223697710482 accuracy_validation: 0.598\n",
      "epochs:  60 loss_train:  1.3651872116954846 accuracy_train: 0.695 loss_val 2.1795809412140112 accuracy_validation: 0.6\n",
      "epochs:  61 loss_train:  1.3572125477413386 accuracy_train: 0.698 loss_val 2.1698993523256718 accuracy_validation: 0.601\n",
      "epochs:  62 loss_train:  1.349520343474867 accuracy_train: 0.697 loss_val 2.1610697991817567 accuracy_validation: 0.603\n",
      "epochs:  63 loss_train:  1.3425594036773096 accuracy_train: 0.698 loss_val 2.15275773861482 accuracy_validation: 0.605\n",
      "epochs:  64 loss_train:  1.3344290919064627 accuracy_train: 0.7 loss_val 2.143528834782606 accuracy_validation: 0.605\n",
      "epochs:  65 loss_train:  1.3268536827514348 accuracy_train: 0.702 loss_val 2.1344864429930097 accuracy_validation: 0.605\n",
      "epochs:  66 loss_train:  1.3195079069645916 accuracy_train: 0.706 loss_val 2.1263975567720994 accuracy_validation: 0.605\n",
      "epochs:  67 loss_train:  1.3122365683680335 accuracy_train: 0.705 loss_val 2.1172657413791796 accuracy_validation: 0.605\n",
      "epochs:  68 loss_train:  1.3054076610249883 accuracy_train: 0.707 loss_val 2.109448389352191 accuracy_validation: 0.606\n",
      "epochs:  69 loss_train:  1.2986392451681266 accuracy_train: 0.707 loss_val 2.1016808958631374 accuracy_validation: 0.606\n",
      "epochs:  70 loss_train:  1.2914665703863397 accuracy_train: 0.707 loss_val 2.093604612919091 accuracy_validation: 0.607\n",
      "epochs:  71 loss_train:  1.2862968354530537 accuracy_train: 0.706 loss_val 2.0872126558807267 accuracy_validation: 0.607\n",
      "epochs:  72 loss_train:  1.2788638569542512 accuracy_train: 0.706 loss_val 2.079279177393129 accuracy_validation: 0.608\n",
      "epochs:  73 loss_train:  1.2726578267882227 accuracy_train: 0.707 loss_val 2.071908593481129 accuracy_validation: 0.61\n",
      "epochs:  74 loss_train:  1.2656581496297414 accuracy_train: 0.706 loss_val 2.0642420911036403 accuracy_validation: 0.609\n",
      "epochs:  75 loss_train:  1.2593016023381747 accuracy_train: 0.706 loss_val 2.056461299859896 accuracy_validation: 0.612\n",
      "epochs:  76 loss_train:  1.2517582892793058 accuracy_train: 0.706 loss_val 2.0482178147482446 accuracy_validation: 0.613\n",
      "epochs:  77 loss_train:  1.246456568565973 accuracy_train: 0.707 loss_val 2.0411706954855102 accuracy_validation: 0.613\n",
      "epochs:  78 loss_train:  1.2401905667239348 accuracy_train: 0.71 loss_val 2.033099004089605 accuracy_validation: 0.617\n",
      "epochs:  79 loss_train:  1.2316842791662261 accuracy_train: 0.712 loss_val 2.0237592505956634 accuracy_validation: 0.617\n",
      "epochs:  80 loss_train:  1.2263374053592835 accuracy_train: 0.713 loss_val 2.016458028132904 accuracy_validation: 0.617\n",
      "epochs:  81 loss_train:  1.2196254288894373 accuracy_train: 0.714 loss_val 2.008212067952942 accuracy_validation: 0.619\n",
      "epochs:  82 loss_train:  1.213869887126994 accuracy_train: 0.716 loss_val 2.000893037916232 accuracy_validation: 0.621\n",
      "epochs:  83 loss_train:  1.2079778420199805 accuracy_train: 0.714 loss_val 1.9937633054511374 accuracy_validation: 0.625\n",
      "epochs:  84 loss_train:  1.2023821701510435 accuracy_train: 0.716 loss_val 1.986869265868076 accuracy_validation: 0.625\n",
      "epochs:  85 loss_train:  1.195897307843294 accuracy_train: 0.717 loss_val 1.979086616247878 accuracy_validation: 0.627\n",
      "epochs:  86 loss_train:  1.1902302158573361 accuracy_train: 0.718 loss_val 1.9714749982262452 accuracy_validation: 0.628\n",
      "epochs:  87 loss_train:  1.185503517560109 accuracy_train: 0.72 loss_val 1.9664272841041508 accuracy_validation: 0.628\n",
      "epochs:  88 loss_train:  1.1791037451837516 accuracy_train: 0.72 loss_val 1.9586437344611412 accuracy_validation: 0.628\n",
      "epochs:  89 loss_train:  1.1743704288643477 accuracy_train: 0.723 loss_val 1.9521548890041656 accuracy_validation: 0.629\n",
      "epochs:  90 loss_train:  1.1689988893041419 accuracy_train: 0.725 loss_val 1.9454852291794074 accuracy_validation: 0.629\n",
      "epochs:  91 loss_train:  1.1639490392335505 accuracy_train: 0.726 loss_val 1.9390470306808736 accuracy_validation: 0.63\n",
      "epochs:  92 loss_train:  1.1584972479518472 accuracy_train: 0.725 loss_val 1.9322847481228314 accuracy_validation: 0.63\n",
      "epochs:  93 loss_train:  1.1533043856883403 accuracy_train: 0.725 loss_val 1.9255076764645702 accuracy_validation: 0.631\n",
      "epochs:  94 loss_train:  1.1479738915857576 accuracy_train: 0.725 loss_val 1.919052488595427 accuracy_validation: 0.632\n",
      "epochs:  95 loss_train:  1.14221958169887 accuracy_train: 0.727 loss_val 1.91229647760426 accuracy_validation: 0.632\n",
      "epochs:  96 loss_train:  1.1380561300921972 accuracy_train: 0.726 loss_val 1.9066947222767294 accuracy_validation: 0.632\n",
      "epochs:  97 loss_train:  1.1326376894159198 accuracy_train: 0.725 loss_val 1.899775256909378 accuracy_validation: 0.633\n",
      "epochs:  98 loss_train:  1.126788011179188 accuracy_train: 0.724 loss_val 1.8934829014576222 accuracy_validation: 0.634\n",
      "epochs:  99 loss_train:  1.1222233002203192 accuracy_train: 0.725 loss_val 1.8876064570266429 accuracy_validation: 0.635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.2485272175364135, 0.577)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (1000, 1, 28, 28)\n",
      "y_train:  (1000, 10)\n",
      "x_val:  (1000, 1, 28, 28)\n",
      "y_val:  (1000, 10)\n",
      "epochs:  0 loss_train:  1.1172568297398922 accuracy_train: 0.726 loss_val 1.8813453298867506 accuracy_validation: 0.635\n",
      "epochs:  1 loss_train:  1.113148644041712 accuracy_train: 0.726 loss_val 1.8764333010170782 accuracy_validation: 0.636\n",
      "epochs:  2 loss_train:  1.1071568984698035 accuracy_train: 0.726 loss_val 1.8689987277743505 accuracy_validation: 0.636\n",
      "epochs:  3 loss_train:  1.1024986547824296 accuracy_train: 0.726 loss_val 1.8638691934919207 accuracy_validation: 0.636\n",
      "epochs:  4 loss_train:  1.0976830437476819 accuracy_train: 0.726 loss_val 1.8577461024783934 accuracy_validation: 0.638\n",
      "epochs:  5 loss_train:  1.0920720307513319 accuracy_train: 0.726 loss_val 1.8510949193273751 accuracy_validation: 0.64\n",
      "epochs:  6 loss_train:  1.0882647371126013 accuracy_train: 0.726 loss_val 1.8460758229288772 accuracy_validation: 0.638\n",
      "epochs:  7 loss_train:  1.0839229185767658 accuracy_train: 0.73 loss_val 1.8411270011571406 accuracy_validation: 0.64\n",
      "epochs:  8 loss_train:  1.0791087869815292 accuracy_train: 0.73 loss_val 1.8354792563790199 accuracy_validation: 0.641\n",
      "epochs:  9 loss_train:  1.0744031272669645 accuracy_train: 0.733 loss_val 1.8295534872745405 accuracy_validation: 0.641\n",
      "epochs:  10 loss_train:  1.0703373029169299 accuracy_train: 0.733 loss_val 1.824157634619385 accuracy_validation: 0.642\n",
      "epochs:  11 loss_train:  1.0652686183516042 accuracy_train: 0.732 loss_val 1.8179165567174098 accuracy_validation: 0.641\n",
      "epochs:  12 loss_train:  1.0618387985704634 accuracy_train: 0.735 loss_val 1.8135209827741876 accuracy_validation: 0.643\n",
      "epochs:  13 loss_train:  1.0574651376845576 accuracy_train: 0.736 loss_val 1.8084069519657586 accuracy_validation: 0.644\n",
      "epochs:  14 loss_train:  1.0532824020674283 accuracy_train: 0.736 loss_val 1.8028789023710945 accuracy_validation: 0.644\n",
      "epochs:  15 loss_train:  1.0492552856331963 accuracy_train: 0.737 loss_val 1.7979968864446327 accuracy_validation: 0.644\n",
      "epochs:  16 loss_train:  1.0447000551647048 accuracy_train: 0.738 loss_val 1.7924384672578335 accuracy_validation: 0.643\n",
      "epochs:  17 loss_train:  1.0404871383152068 accuracy_train: 0.74 loss_val 1.78754242642202 accuracy_validation: 0.644\n",
      "epochs:  18 loss_train:  1.0366901358807172 accuracy_train: 0.739 loss_val 1.782343875245851 accuracy_validation: 0.644\n",
      "epochs:  19 loss_train:  1.031370771040186 accuracy_train: 0.739 loss_val 1.7765658912937665 accuracy_validation: 0.646\n",
      "epochs:  20 loss_train:  1.0276800249987181 accuracy_train: 0.74 loss_val 1.7720748020309207 accuracy_validation: 0.646\n",
      "epochs:  21 loss_train:  1.0227623616403145 accuracy_train: 0.741 loss_val 1.7662707146563654 accuracy_validation: 0.647\n",
      "epochs:  22 loss_train:  1.0186394718043839 accuracy_train: 0.74 loss_val 1.7611813366113056 accuracy_validation: 0.648\n",
      "epochs:  23 loss_train:  1.0151607826671667 accuracy_train: 0.741 loss_val 1.7566498794417262 accuracy_validation: 0.648\n",
      "epochs:  24 loss_train:  1.0106266593792184 accuracy_train: 0.743 loss_val 1.7511368047685727 accuracy_validation: 0.648\n",
      "epochs:  25 loss_train:  1.0064763256567948 accuracy_train: 0.74 loss_val 1.7461704837787175 accuracy_validation: 0.649\n",
      "epochs:  26 loss_train:  1.0027612514071027 accuracy_train: 0.742 loss_val 1.7413547344154012 accuracy_validation: 0.65\n",
      "epochs:  27 loss_train:  0.9980955497758854 accuracy_train: 0.741 loss_val 1.7357490745154824 accuracy_validation: 0.651\n",
      "epochs:  28 loss_train:  0.9940179379184593 accuracy_train: 0.742 loss_val 1.7308274635499614 accuracy_validation: 0.651\n",
      "epochs:  29 loss_train:  0.9903950490706014 accuracy_train: 0.743 loss_val 1.7261125986688455 accuracy_validation: 0.651\n",
      "epochs:  30 loss_train:  0.9861008527873775 accuracy_train: 0.744 loss_val 1.7211070267245954 accuracy_validation: 0.65\n",
      "epochs:  31 loss_train:  0.9824218455206137 accuracy_train: 0.744 loss_val 1.7163446376492606 accuracy_validation: 0.65\n",
      "epochs:  32 loss_train:  0.9785058440064779 accuracy_train: 0.744 loss_val 1.7109902111122741 accuracy_validation: 0.651\n",
      "epochs:  33 loss_train:  0.9746811559599826 accuracy_train: 0.744 loss_val 1.7062442201710992 accuracy_validation: 0.65\n",
      "epochs:  34 loss_train:  0.9708446094105638 accuracy_train: 0.744 loss_val 1.7014767776711082 accuracy_validation: 0.65\n",
      "epochs:  35 loss_train:  0.9672048594735726 accuracy_train: 0.745 loss_val 1.696995762117025 accuracy_validation: 0.65\n",
      "epochs:  36 loss_train:  0.9629003143823481 accuracy_train: 0.745 loss_val 1.6921340280460795 accuracy_validation: 0.65\n",
      "epochs:  37 loss_train:  0.9598134106683669 accuracy_train: 0.746 loss_val 1.6876914357891184 accuracy_validation: 0.651\n",
      "epochs:  38 loss_train:  0.9559145945595059 accuracy_train: 0.747 loss_val 1.6828513985134768 accuracy_validation: 0.651\n",
      "epochs:  39 loss_train:  0.9523404839714605 accuracy_train: 0.748 loss_val 1.6787079862701255 accuracy_validation: 0.652\n",
      "epochs:  40 loss_train:  0.9476518213118049 accuracy_train: 0.748 loss_val 1.6736772039393677 accuracy_validation: 0.654\n",
      "epochs:  41 loss_train:  0.9442276008947724 accuracy_train: 0.748 loss_val 1.6697679578040807 accuracy_validation: 0.656\n",
      "epochs:  42 loss_train:  0.9407070050406449 accuracy_train: 0.748 loss_val 1.6652510484035714 accuracy_validation: 0.657\n",
      "epochs:  43 loss_train:  0.9370565271078908 accuracy_train: 0.748 loss_val 1.6610686337489646 accuracy_validation: 0.656\n",
      "epochs:  44 loss_train:  0.9337178339699983 accuracy_train: 0.748 loss_val 1.65677550276274 accuracy_validation: 0.656\n",
      "epochs:  45 loss_train:  0.9299934252715674 accuracy_train: 0.749 loss_val 1.652334966594083 accuracy_validation: 0.657\n",
      "epochs:  46 loss_train:  0.9267236858052154 accuracy_train: 0.75 loss_val 1.6486631250469492 accuracy_validation: 0.657\n",
      "epochs:  47 loss_train:  0.9236071794532019 accuracy_train: 0.751 loss_val 1.6445916370759022 accuracy_validation: 0.657\n",
      "epochs:  48 loss_train:  0.9199878246564929 accuracy_train: 0.752 loss_val 1.6407243546900072 accuracy_validation: 0.657\n",
      "epochs:  49 loss_train:  0.9171082616798502 accuracy_train: 0.752 loss_val 1.6372925677643357 accuracy_validation: 0.657\n",
      "epochs:  50 loss_train:  0.9131808955023973 accuracy_train: 0.75 loss_val 1.6330394554682832 accuracy_validation: 0.658\n",
      "epochs:  51 loss_train:  0.9101044242409022 accuracy_train: 0.751 loss_val 1.6291527587192685 accuracy_validation: 0.659\n",
      "epochs:  52 loss_train:  0.9067655783852845 accuracy_train: 0.752 loss_val 1.6253379302117106 accuracy_validation: 0.66\n",
      "epochs:  53 loss_train:  0.9036845528216887 accuracy_train: 0.751 loss_val 1.6216877489982522 accuracy_validation: 0.658\n",
      "epochs:  54 loss_train:  0.9002704569486586 accuracy_train: 0.753 loss_val 1.6178906632318233 accuracy_validation: 0.659\n",
      "epochs:  55 loss_train:  0.8971574897097234 accuracy_train: 0.753 loss_val 1.6145752008449747 accuracy_validation: 0.66\n",
      "epochs:  56 loss_train:  0.8938797009126519 accuracy_train: 0.756 loss_val 1.610869387466929 accuracy_validation: 0.661\n",
      "epochs:  57 loss_train:  0.8908374217894433 accuracy_train: 0.758 loss_val 1.6072282134150406 accuracy_validation: 0.66\n",
      "epochs:  58 loss_train:  0.8875723415207617 accuracy_train: 0.758 loss_val 1.6033571505096464 accuracy_validation: 0.663\n",
      "epochs:  59 loss_train:  0.884400902552327 accuracy_train: 0.76 loss_val 1.600303026629203 accuracy_validation: 0.664\n",
      "epochs:  60 loss_train:  0.8815891182237681 accuracy_train: 0.759 loss_val 1.5966525437636045 accuracy_validation: 0.664\n",
      "epochs:  61 loss_train:  0.8783950390916827 accuracy_train: 0.761 loss_val 1.5932638782153772 accuracy_validation: 0.665\n",
      "epochs:  62 loss_train:  0.8753340881900876 accuracy_train: 0.761 loss_val 1.589778801391028 accuracy_validation: 0.665\n",
      "epochs:  63 loss_train:  0.8721259165290055 accuracy_train: 0.762 loss_val 1.5867615551311172 accuracy_validation: 0.666\n",
      "epochs:  64 loss_train:  0.8692321384877518 accuracy_train: 0.761 loss_val 1.5834315172371793 accuracy_validation: 0.665\n",
      "epochs:  65 loss_train:  0.8666131545071762 accuracy_train: 0.761 loss_val 1.5805452668360587 accuracy_validation: 0.664\n",
      "epochs:  66 loss_train:  0.8638589650345199 accuracy_train: 0.762 loss_val 1.576798111559698 accuracy_validation: 0.667\n",
      "epochs:  67 loss_train:  0.8611739699418891 accuracy_train: 0.763 loss_val 1.5739844225956166 accuracy_validation: 0.667\n",
      "epochs:  68 loss_train:  0.8579244043690863 accuracy_train: 0.765 loss_val 1.5711509752795283 accuracy_validation: 0.668\n",
      "epochs:  69 loss_train:  0.8550147435922194 accuracy_train: 0.766 loss_val 1.5670442506300906 accuracy_validation: 0.668\n",
      "epochs:  70 loss_train:  0.8525993774843854 accuracy_train: 0.767 loss_val 1.5650430132187754 accuracy_validation: 0.667\n",
      "epochs:  71 loss_train:  0.8492695054922524 accuracy_train: 0.769 loss_val 1.560957903741994 accuracy_validation: 0.668\n",
      "epochs:  72 loss_train:  0.8463419960018198 accuracy_train: 0.771 loss_val 1.5582183612351583 accuracy_validation: 0.668\n",
      "epochs:  73 loss_train:  0.8440577291349097 accuracy_train: 0.771 loss_val 1.5559725219682194 accuracy_validation: 0.669\n",
      "epochs:  74 loss_train:  0.8407877720303454 accuracy_train: 0.771 loss_val 1.5526179477934114 accuracy_validation: 0.669\n",
      "epochs:  75 loss_train:  0.8383660125797632 accuracy_train: 0.772 loss_val 1.5498688904634141 accuracy_validation: 0.669\n",
      "epochs:  76 loss_train:  0.8352295667733476 accuracy_train: 0.772 loss_val 1.546421516309183 accuracy_validation: 0.669\n",
      "epochs:  77 loss_train:  0.8326272835332259 accuracy_train: 0.774 loss_val 1.543516200126202 accuracy_validation: 0.669\n",
      "epochs:  78 loss_train:  0.8297128683859241 accuracy_train: 0.775 loss_val 1.5407014871273066 accuracy_validation: 0.673\n",
      "epochs:  79 loss_train:  0.8268121810978354 accuracy_train: 0.776 loss_val 1.5374090875522015 accuracy_validation: 0.671\n",
      "epochs:  80 loss_train:  0.8246880031222336 accuracy_train: 0.778 loss_val 1.5355067666702098 accuracy_validation: 0.672\n",
      "epochs:  81 loss_train:  0.8218587560941557 accuracy_train: 0.778 loss_val 1.532183776097742 accuracy_validation: 0.671\n",
      "epochs:  82 loss_train:  0.8189035282873655 accuracy_train: 0.778 loss_val 1.529265692919127 accuracy_validation: 0.672\n",
      "epochs:  83 loss_train:  0.8164078017715003 accuracy_train: 0.778 loss_val 1.527143189647772 accuracy_validation: 0.673\n",
      "epochs:  84 loss_train:  0.8137849863797626 accuracy_train: 0.779 loss_val 1.5238919045107924 accuracy_validation: 0.673\n",
      "epochs:  85 loss_train:  0.8115162435649209 accuracy_train: 0.779 loss_val 1.5213946947448098 accuracy_validation: 0.673\n",
      "epochs:  86 loss_train:  0.8085524837119221 accuracy_train: 0.78 loss_val 1.5184148821466965 accuracy_validation: 0.674\n",
      "epochs:  87 loss_train:  0.8059266525514117 accuracy_train: 0.779 loss_val 1.514981439962188 accuracy_validation: 0.673\n",
      "epochs:  88 loss_train:  0.8037675048587276 accuracy_train: 0.781 loss_val 1.5131495801222936 accuracy_validation: 0.675\n",
      "epochs:  89 loss_train:  0.8012408357348391 accuracy_train: 0.781 loss_val 1.510461859251702 accuracy_validation: 0.674\n",
      "epochs:  90 loss_train:  0.7989065881341006 accuracy_train: 0.784 loss_val 1.5069515553876953 accuracy_validation: 0.674\n",
      "epochs:  91 loss_train:  0.7965754318175174 accuracy_train: 0.783 loss_val 1.5049117526266693 accuracy_validation: 0.676\n",
      "epochs:  92 loss_train:  0.7937760946115113 accuracy_train: 0.784 loss_val 1.5016757011309312 accuracy_validation: 0.676\n",
      "epochs:  93 loss_train:  0.7915463422769806 accuracy_train: 0.784 loss_val 1.4987587622242686 accuracy_validation: 0.676\n",
      "epochs:  94 loss_train:  0.7890022011888365 accuracy_train: 0.784 loss_val 1.4964854727836852 accuracy_validation: 0.676\n",
      "epochs:  95 loss_train:  0.7865980118786435 accuracy_train: 0.785 loss_val 1.4935220285409714 accuracy_validation: 0.679\n",
      "epochs:  96 loss_train:  0.7844756184172336 accuracy_train: 0.785 loss_val 1.4910341721766935 accuracy_validation: 0.679\n",
      "epochs:  97 loss_train:  0.7816923452836648 accuracy_train: 0.786 loss_val 1.4881989845335628 accuracy_validation: 0.679\n",
      "epochs:  98 loss_train:  0.7797554184979583 accuracy_train: 0.787 loss_val 1.4858036370905794 accuracy_validation: 0.679\n",
      "epochs:  99 loss_train:  0.7773449106137128 accuracy_train: 0.787 loss_val 1.4834487562573389 accuracy_validation: 0.679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.7953899424471678, 0.624)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (1000, 1, 28, 28)\n",
      "y_train:  (1000, 10)\n",
      "x_val:  (1000, 1, 28, 28)\n",
      "y_val:  (1000, 10)\n",
      "epochs:  0 loss_train:  0.774672698140634 accuracy_train: 0.788 loss_val 1.4804486865764215 accuracy_validation: 0.68\n",
      "epochs:  1 loss_train:  0.7725060163156455 accuracy_train: 0.789 loss_val 1.4786179499877699 accuracy_validation: 0.681\n",
      "epochs:  2 loss_train:  0.7704288447271124 accuracy_train: 0.79 loss_val 1.4759545070532374 accuracy_validation: 0.681\n",
      "epochs:  3 loss_train:  0.7681234079569984 accuracy_train: 0.79 loss_val 1.4736814072225175 accuracy_validation: 0.681\n",
      "epochs:  4 loss_train:  0.7658426317889586 accuracy_train: 0.791 loss_val 1.471156504763075 accuracy_validation: 0.682\n",
      "epochs:  5 loss_train:  0.763283803891671 accuracy_train: 0.79 loss_val 1.4683641237974805 accuracy_validation: 0.682\n",
      "epochs:  6 loss_train:  0.7611683771004119 accuracy_train: 0.789 loss_val 1.4658804894779087 accuracy_validation: 0.682\n",
      "epochs:  7 loss_train:  0.7592132335398685 accuracy_train: 0.789 loss_val 1.4638757321068356 accuracy_validation: 0.682\n",
      "epochs:  8 loss_train:  0.7565495286900062 accuracy_train: 0.791 loss_val 1.4607604759196202 accuracy_validation: 0.683\n",
      "epochs:  9 loss_train:  0.7544469010775865 accuracy_train: 0.792 loss_val 1.4582181079586494 accuracy_validation: 0.685\n",
      "epochs:  10 loss_train:  0.7522678412038694 accuracy_train: 0.793 loss_val 1.4561024617365448 accuracy_validation: 0.685\n",
      "epochs:  11 loss_train:  0.750071495953081 accuracy_train: 0.793 loss_val 1.4533895069012805 accuracy_validation: 0.688\n",
      "epochs:  12 loss_train:  0.7480269535425607 accuracy_train: 0.793 loss_val 1.4512450267373846 accuracy_validation: 0.688\n",
      "epochs:  13 loss_train:  0.7461884623887901 accuracy_train: 0.794 loss_val 1.4485546534961187 accuracy_validation: 0.691\n",
      "epochs:  14 loss_train:  0.7437851026830963 accuracy_train: 0.794 loss_val 1.446119353683852 accuracy_validation: 0.688\n",
      "epochs:  15 loss_train:  0.7420254382261403 accuracy_train: 0.796 loss_val 1.4438948338339985 accuracy_validation: 0.691\n",
      "epochs:  16 loss_train:  0.7395792188251424 accuracy_train: 0.796 loss_val 1.4414795483673506 accuracy_validation: 0.691\n",
      "epochs:  17 loss_train:  0.7376178387313147 accuracy_train: 0.797 loss_val 1.4385795809448878 accuracy_validation: 0.693\n",
      "epochs:  18 loss_train:  0.7355214353976438 accuracy_train: 0.796 loss_val 1.4367241012837029 accuracy_validation: 0.691\n",
      "epochs:  19 loss_train:  0.733626666489308 accuracy_train: 0.797 loss_val 1.434027130517313 accuracy_validation: 0.692\n",
      "epochs:  20 loss_train:  0.7316226215504444 accuracy_train: 0.797 loss_val 1.4321537084221576 accuracy_validation: 0.692\n",
      "epochs:  21 loss_train:  0.7293173401522087 accuracy_train: 0.798 loss_val 1.429970681238666 accuracy_validation: 0.692\n",
      "epochs:  22 loss_train:  0.7273705594696273 accuracy_train: 0.798 loss_val 1.4274206038843016 accuracy_validation: 0.694\n",
      "epochs:  23 loss_train:  0.7252030226537581 accuracy_train: 0.798 loss_val 1.4250791612509024 accuracy_validation: 0.696\n",
      "epochs:  24 loss_train:  0.7234649798086962 accuracy_train: 0.798 loss_val 1.4235272805451895 accuracy_validation: 0.697\n",
      "epochs:  25 loss_train:  0.7210645144940118 accuracy_train: 0.801 loss_val 1.4210149842822446 accuracy_validation: 0.699\n",
      "epochs:  26 loss_train:  0.7195071194423835 accuracy_train: 0.798 loss_val 1.4197223833981016 accuracy_validation: 0.698\n",
      "epochs:  27 loss_train:  0.7167480928188846 accuracy_train: 0.804 loss_val 1.4173449563689897 accuracy_validation: 0.699\n",
      "epochs:  28 loss_train:  0.715060162966526 accuracy_train: 0.802 loss_val 1.4158356120841935 accuracy_validation: 0.699\n",
      "epochs:  29 loss_train:  0.7134826956362216 accuracy_train: 0.802 loss_val 1.4135025836978248 accuracy_validation: 0.699\n",
      "epochs:  30 loss_train:  0.7114312675992129 accuracy_train: 0.803 loss_val 1.4119678406930407 accuracy_validation: 0.699\n",
      "epochs:  31 loss_train:  0.7090994928910807 accuracy_train: 0.804 loss_val 1.4098945863280656 accuracy_validation: 0.7\n",
      "epochs:  32 loss_train:  0.7074898575045023 accuracy_train: 0.803 loss_val 1.4083548307050178 accuracy_validation: 0.7\n",
      "epochs:  33 loss_train:  0.7048878625710441 accuracy_train: 0.805 loss_val 1.4056483846597234 accuracy_validation: 0.703\n",
      "epochs:  34 loss_train:  0.7035993775935755 accuracy_train: 0.803 loss_val 1.4043034435184238 accuracy_validation: 0.702\n",
      "epochs:  35 loss_train:  0.7013112446542131 accuracy_train: 0.805 loss_val 1.4022691709909176 accuracy_validation: 0.703\n",
      "epochs:  36 loss_train:  0.6996716143485023 accuracy_train: 0.805 loss_val 1.401482682681474 accuracy_validation: 0.703\n",
      "epochs:  37 loss_train:  0.6974187411639888 accuracy_train: 0.807 loss_val 1.3994441090609886 accuracy_validation: 0.703\n",
      "epochs:  38 loss_train:  0.6953722030844021 accuracy_train: 0.806 loss_val 1.3975533000546585 accuracy_validation: 0.703\n",
      "epochs:  39 loss_train:  0.6939041519965173 accuracy_train: 0.806 loss_val 1.396384866718532 accuracy_validation: 0.704\n",
      "epochs:  40 loss_train:  0.6915312437845214 accuracy_train: 0.808 loss_val 1.3939033646829246 accuracy_validation: 0.703\n",
      "epochs:  41 loss_train:  0.6895615230193504 accuracy_train: 0.807 loss_val 1.3926975121651541 accuracy_validation: 0.707\n",
      "epochs:  42 loss_train:  0.687609117370673 accuracy_train: 0.808 loss_val 1.3901075781392327 accuracy_validation: 0.707\n",
      "epochs:  43 loss_train:  0.6858018408220047 accuracy_train: 0.81 loss_val 1.3882387268005956 accuracy_validation: 0.708\n",
      "epochs:  44 loss_train:  0.6839419856092014 accuracy_train: 0.811 loss_val 1.3864460138738237 accuracy_validation: 0.709\n",
      "epochs:  45 loss_train:  0.6821657271837612 accuracy_train: 0.812 loss_val 1.3848290456689047 accuracy_validation: 0.708\n",
      "epochs:  46 loss_train:  0.6802123809742454 accuracy_train: 0.813 loss_val 1.38248433293948 accuracy_validation: 0.708\n",
      "epochs:  47 loss_train:  0.6784417917845372 accuracy_train: 0.815 loss_val 1.3813056652595055 accuracy_validation: 0.707\n",
      "epochs:  48 loss_train:  0.6764030437549895 accuracy_train: 0.814 loss_val 1.378867886050232 accuracy_validation: 0.708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtest(x_test,y_test)\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\CNN_autograd.py:101\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, x_train, y_train, x_val, y_val, batch_size, epochs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# if(e % 5 == 0):\u001b[39;00m\n\u001b[0;32m    100\u001b[0m (loss_train, accuracy_train) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(x_train, y_train)\n\u001b[1;32m--> 101\u001b[0m (loss_val, accuracy_val) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs: \u001b[39m\u001b[38;5;124m\"\u001b[39m, e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_train: \u001b[39m\u001b[38;5;124m\"\u001b[39m, loss_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy_train:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_train,\n\u001b[0;32m    103\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_val\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss_val, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy_validation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_val)\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\CNN_autograd.py:109\u001b[0m, in \u001b[0;36mModel.test\u001b[1;34m(self, x_test, y_test)\u001b[0m\n\u001b[0;32m    107\u001b[0m output \u001b[38;5;241m=\u001b[39m Tensor(x_test, requires_grad\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layers \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 109\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(y_test \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(output\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)) \u001b[38;5;241m/\u001b[39m y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    113\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_accuracy(np\u001b[38;5;241m.\u001b[39margmax(output\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mT,\u001b[38;5;241m0\u001b[39m),np\u001b[38;5;241m.\u001b[39margmax(y_test\u001b[38;5;241m.\u001b[39mT,\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\CNN_autograd.py:57\u001b[0m, in \u001b[0;36mMaxPoolingLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m): \u001b[38;5;66;03m# input 4 chiu l 1 tensor\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpooling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:301\u001b[0m, in \u001b[0;36mTensor.maxpooling\u001b[1;34m(self, pool_size)\u001b[0m\n\u001b[0;32m    299\u001b[0m                 w_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(w_start \u001b[38;5;241m+\u001b[39m pool_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m    300\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m h_start \u001b[38;5;241m<\u001b[39m h_end \u001b[38;5;129;01mand\u001b[39;00m w_start \u001b[38;5;241m<\u001b[39m w_end:\n\u001b[1;32m--> 301\u001b[0m                     output[n, c, i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mh_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mw_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m result \u001b[38;5;241m=\u001b[39m Tensor(output, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad, depends_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m], operator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxpool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m(grad):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 32, 32) (1000, 10)\n",
      "(1000, 3, 32, 32) (1000, 10)\n",
      "(10000, 3, 32, 32) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from MLP_autograd import *\n",
    "from CNN_autograd import *\n",
    "from keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data()\n",
    "x_train = x_train.transpose(0, 3, 1, 2)\n",
    "x_test = x_test.transpose(0, 3, 1, 2)\n",
    "def one_hot(Y):\n",
    "  one_hot_Y = np.zeros((Y.size,np.max(Y) + 1))\n",
    "  one_hot_Y[np.arange(Y.size),Y] = 1\n",
    "  return one_hot_Y\n",
    "\n",
    "\n",
    "y_train = one_hot(y_train.T)\n",
    "y_test = one_hot(y_test)\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "x_validation = x_train[49000:50000]\n",
    "y_validation = y_train[49000:50000,:]\n",
    "x_train = x_train[0:1000]\n",
    "y_train = y_train[0:1000,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_validation.shape,y_validation.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (1000, 3, 32, 32)\n",
      "y_train:  (1000, 10)\n",
      "x_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000, 10)\n",
      "epochs:  0 loss_train:  11.613791877719578 accuracy_train: 0.105 loss_val 11.518412170549626 accuracy_validation: 0.097\n",
      "epochs:  1 loss_train:  11.302936453837155 accuracy_train: 0.097 loss_val 11.180641131938529 accuracy_validation: 0.104\n",
      "epochs:  2 loss_train:  10.93579199190302 accuracy_train: 0.103 loss_val 10.88995385727677 accuracy_validation: 0.114\n",
      "epochs:  3 loss_train:  10.58942567710182 accuracy_train: 0.111 loss_val 10.53036321554819 accuracy_validation: 0.126\n",
      "epochs:  4 loss_train:  10.212947832940285 accuracy_train: 0.125 loss_val 10.274328810746677 accuracy_validation: 0.125\n",
      "epochs:  5 loss_train:  9.86976117085449 accuracy_train: 0.128 loss_val 10.056808338990384 accuracy_validation: 0.128\n",
      "epochs:  6 loss_train:  9.591825836608008 accuracy_train: 0.127 loss_val 9.791222057948586 accuracy_validation: 0.125\n",
      "epochs:  7 loss_train:  9.444124551083164 accuracy_train: 0.126 loss_val 9.671063603933897 accuracy_validation: 0.109\n",
      "epochs:  8 loss_train:  9.158842408602988 accuracy_train: 0.115 loss_val 9.364828475217314 accuracy_validation: 0.116\n",
      "epochs:  9 loss_train:  8.78572879769014 accuracy_train: 0.119 loss_val 9.051559312189022 accuracy_validation: 0.117\n",
      "epochs:  10 loss_train:  8.593890592649313 accuracy_train: 0.123 loss_val 8.895182451663423 accuracy_validation: 0.111\n",
      "epochs:  11 loss_train:  8.26106500349503 accuracy_train: 0.125 loss_val 8.603230376609975 accuracy_validation: 0.114\n",
      "epochs:  12 loss_train:  8.022916917494857 accuracy_train: 0.126 loss_val 8.399254120732536 accuracy_validation: 0.116\n",
      "epochs:  13 loss_train:  7.744891205575394 accuracy_train: 0.134 loss_val 8.163933757660164 accuracy_validation: 0.121\n",
      "epochs:  14 loss_train:  7.524894827224369 accuracy_train: 0.134 loss_val 7.942896959721464 accuracy_validation: 0.115\n",
      "epochs:  15 loss_train:  7.249273361701802 accuracy_train: 0.128 loss_val 7.687304978949908 accuracy_validation: 0.128\n",
      "epochs:  16 loss_train:  7.042263797751848 accuracy_train: 0.129 loss_val 7.4626226785397725 accuracy_validation: 0.126\n",
      "epochs:  17 loss_train:  6.823994187253953 accuracy_train: 0.13 loss_val 7.223522969999846 accuracy_validation: 0.118\n",
      "epochs:  18 loss_train:  6.603452646142525 accuracy_train: 0.135 loss_val 6.989321366300951 accuracy_validation: 0.117\n",
      "epochs:  19 loss_train:  6.447389668506532 accuracy_train: 0.135 loss_val 6.822203195767203 accuracy_validation: 0.119\n",
      "epochs:  20 loss_train:  6.217303891814749 accuracy_train: 0.137 loss_val 6.5905795401556455 accuracy_validation: 0.118\n",
      "epochs:  21 loss_train:  6.052296835209599 accuracy_train: 0.136 loss_val 6.415729029925235 accuracy_validation: 0.122\n",
      "epochs:  22 loss_train:  5.870556453053659 accuracy_train: 0.146 loss_val 6.239476396419053 accuracy_validation: 0.122\n",
      "epochs:  23 loss_train:  5.749038349791328 accuracy_train: 0.145 loss_val 6.111378741959497 accuracy_validation: 0.126\n",
      "epochs:  24 loss_train:  5.633585840651904 accuracy_train: 0.142 loss_val 5.980504784103537 accuracy_validation: 0.125\n",
      "epochs:  25 loss_train:  5.527373951177231 accuracy_train: 0.14 loss_val 5.850526429407142 accuracy_validation: 0.127\n",
      "epochs:  26 loss_train:  5.413275104744305 accuracy_train: 0.138 loss_val 5.695740240448188 accuracy_validation: 0.128\n",
      "epochs:  27 loss_train:  5.293687555674094 accuracy_train: 0.139 loss_val 5.551830541041557 accuracy_validation: 0.131\n",
      "epochs:  28 loss_train:  5.189134675136506 accuracy_train: 0.137 loss_val 5.439819802475458 accuracy_validation: 0.125\n",
      "epochs:  29 loss_train:  5.108134686854816 accuracy_train: 0.132 loss_val 5.343416705753215 accuracy_validation: 0.124\n",
      "epochs:  30 loss_train:  4.986056466566158 accuracy_train: 0.139 loss_val 5.2170000522283475 accuracy_validation: 0.127\n",
      "epochs:  31 loss_train:  4.890836882365785 accuracy_train: 0.141 loss_val 5.121628395136359 accuracy_validation: 0.128\n",
      "epochs:  32 loss_train:  4.811279605404767 accuracy_train: 0.14 loss_val 5.031953666054111 accuracy_validation: 0.13\n",
      "epochs:  33 loss_train:  4.751253900428186 accuracy_train: 0.138 loss_val 4.966703319377092 accuracy_validation: 0.128\n",
      "epochs:  34 loss_train:  4.699412032196208 accuracy_train: 0.138 loss_val 4.904676914040735 accuracy_validation: 0.126\n",
      "epochs:  35 loss_train:  4.644545199462102 accuracy_train: 0.136 loss_val 4.845559477666408 accuracy_validation: 0.125\n",
      "epochs:  36 loss_train:  4.610645156050656 accuracy_train: 0.134 loss_val 4.808565476240433 accuracy_validation: 0.123\n",
      "epochs:  37 loss_train:  4.531226271002444 accuracy_train: 0.139 loss_val 4.734211346898743 accuracy_validation: 0.131\n",
      "epochs:  38 loss_train:  4.463145321159015 accuracy_train: 0.139 loss_val 4.670077791169395 accuracy_validation: 0.134\n",
      "epochs:  39 loss_train:  4.416733094017033 accuracy_train: 0.136 loss_val 4.624352792669743 accuracy_validation: 0.132\n",
      "epochs:  40 loss_train:  4.353521562182749 accuracy_train: 0.139 loss_val 4.562086817872567 accuracy_validation: 0.128\n",
      "epochs:  41 loss_train:  4.3141542321657775 accuracy_train: 0.143 loss_val 4.523210055662455 accuracy_validation: 0.128\n",
      "epochs:  42 loss_train:  4.263262207340531 accuracy_train: 0.142 loss_val 4.473801179662688 accuracy_validation: 0.125\n",
      "epochs:  43 loss_train:  4.234598708058887 accuracy_train: 0.142 loss_val 4.441165861542358 accuracy_validation: 0.128\n",
      "epochs:  44 loss_train:  4.184439530555278 accuracy_train: 0.143 loss_val 4.39023845968273 accuracy_validation: 0.126\n",
      "epochs:  45 loss_train:  4.143469300926581 accuracy_train: 0.14 loss_val 4.347675495933599 accuracy_validation: 0.126\n",
      "epochs:  46 loss_train:  4.110896621103803 accuracy_train: 0.141 loss_val 4.313361000437824 accuracy_validation: 0.125\n",
      "epochs:  47 loss_train:  4.090673223147766 accuracy_train: 0.145 loss_val 4.289118797242163 accuracy_validation: 0.122\n",
      "epochs:  48 loss_train:  4.0551757256889225 accuracy_train: 0.141 loss_val 4.252195544317494 accuracy_validation: 0.122\n",
      "epochs:  49 loss_train:  4.020245097297995 accuracy_train: 0.138 loss_val 4.213617600870499 accuracy_validation: 0.123\n",
      "epochs:  50 loss_train:  3.9798565734856868 accuracy_train: 0.137 loss_val 4.172643388107615 accuracy_validation: 0.122\n",
      "epochs:  51 loss_train:  3.9610925070947247 accuracy_train: 0.134 loss_val 4.151809584411103 accuracy_validation: 0.125\n",
      "epochs:  52 loss_train:  3.9361951230624825 accuracy_train: 0.131 loss_val 4.124049141919892 accuracy_validation: 0.125\n",
      "epochs:  53 loss_train:  3.9210788732924238 accuracy_train: 0.132 loss_val 4.104571104616774 accuracy_validation: 0.127\n",
      "epochs:  54 loss_train:  3.8830867861621163 accuracy_train: 0.13 loss_val 4.066531203434698 accuracy_validation: 0.126\n",
      "epochs:  55 loss_train:  3.8602942608453814 accuracy_train: 0.131 loss_val 4.042011045507028 accuracy_validation: 0.124\n",
      "epochs:  56 loss_train:  3.8177075743719335 accuracy_train: 0.13 loss_val 4.001675687510746 accuracy_validation: 0.122\n",
      "epochs:  57 loss_train:  3.791174525368663 accuracy_train: 0.129 loss_val 3.974053332605968 accuracy_validation: 0.121\n",
      "epochs:  58 loss_train:  3.7666318808168877 accuracy_train: 0.132 loss_val 3.948650266622628 accuracy_validation: 0.124\n",
      "epochs:  59 loss_train:  3.7494268833606257 accuracy_train: 0.133 loss_val 3.9284741011243685 accuracy_validation: 0.123\n",
      "epochs:  60 loss_train:  3.732972118306832 accuracy_train: 0.136 loss_val 3.9102093077774525 accuracy_validation: 0.123\n",
      "epochs:  61 loss_train:  3.7001804440085917 accuracy_train: 0.136 loss_val 3.8785297149878653 accuracy_validation: 0.126\n",
      "epochs:  62 loss_train:  3.6825729264247085 accuracy_train: 0.136 loss_val 3.859964589234626 accuracy_validation: 0.123\n",
      "epochs:  63 loss_train:  3.6591380978367516 accuracy_train: 0.135 loss_val 3.8383707844410506 accuracy_validation: 0.125\n",
      "epochs:  64 loss_train:  3.633544290441521 accuracy_train: 0.14 loss_val 3.8145638669551714 accuracy_validation: 0.127\n",
      "epochs:  65 loss_train:  3.6120577075721 accuracy_train: 0.14 loss_val 3.7955315841675503 accuracy_validation: 0.127\n",
      "epochs:  66 loss_train:  3.5913135745423674 accuracy_train: 0.138 loss_val 3.7760489400791926 accuracy_validation: 0.127\n",
      "epochs:  67 loss_train:  3.57488247373522 accuracy_train: 0.14 loss_val 3.76046764672108 accuracy_validation: 0.128\n",
      "epochs:  68 loss_train:  3.558196617368324 accuracy_train: 0.141 loss_val 3.7439391948449194 accuracy_validation: 0.128\n",
      "epochs:  69 loss_train:  3.5465782666133276 accuracy_train: 0.141 loss_val 3.7313199061123923 accuracy_validation: 0.128\n",
      "epochs:  70 loss_train:  3.5271690074687934 accuracy_train: 0.145 loss_val 3.712432695133093 accuracy_validation: 0.129\n",
      "epochs:  71 loss_train:  3.510425795083078 accuracy_train: 0.144 loss_val 3.695393623225345 accuracy_validation: 0.13\n",
      "epochs:  72 loss_train:  3.4936971149288647 accuracy_train: 0.144 loss_val 3.679284516258807 accuracy_validation: 0.131\n",
      "epochs:  73 loss_train:  3.4753532833265024 accuracy_train: 0.143 loss_val 3.661522120688978 accuracy_validation: 0.131\n",
      "epochs:  74 loss_train:  3.4566465537138 accuracy_train: 0.143 loss_val 3.6442206874190712 accuracy_validation: 0.131\n",
      "epochs:  75 loss_train:  3.4375824625888476 accuracy_train: 0.143 loss_val 3.6263745626900694 accuracy_validation: 0.131\n",
      "epochs:  76 loss_train:  3.413931114581468 accuracy_train: 0.144 loss_val 3.6049391902048336 accuracy_validation: 0.13\n",
      "epochs:  77 loss_train:  3.399714007129746 accuracy_train: 0.146 loss_val 3.590644758491893 accuracy_validation: 0.13\n",
      "epochs:  78 loss_train:  3.3857344126317286 accuracy_train: 0.147 loss_val 3.576407718499927 accuracy_validation: 0.129\n",
      "epochs:  79 loss_train:  3.382277040136327 accuracy_train: 0.146 loss_val 3.571465562673187 accuracy_validation: 0.128\n",
      "epochs:  80 loss_train:  3.3660539801706455 accuracy_train: 0.148 loss_val 3.5558490668563945 accuracy_validation: 0.128\n",
      "epochs:  81 loss_train:  3.3535345089666686 accuracy_train: 0.146 loss_val 3.5436148324578407 accuracy_validation: 0.133\n",
      "epochs:  82 loss_train:  3.339784903189492 accuracy_train: 0.146 loss_val 3.5306519413791775 accuracy_validation: 0.134\n",
      "epochs:  83 loss_train:  3.322538735587451 accuracy_train: 0.149 loss_val 3.5149556804124478 accuracy_validation: 0.132\n",
      "epochs:  84 loss_train:  3.2988966668229596 accuracy_train: 0.148 loss_val 3.492950180160116 accuracy_validation: 0.132\n",
      "epochs:  85 loss_train:  3.2864427583885565 accuracy_train: 0.148 loss_val 3.481217319106246 accuracy_validation: 0.133\n",
      "epochs:  86 loss_train:  3.272536190018058 accuracy_train: 0.151 loss_val 3.4676911972925137 accuracy_validation: 0.133\n",
      "epochs:  87 loss_train:  3.2517334659395156 accuracy_train: 0.148 loss_val 3.448147258544755 accuracy_validation: 0.133\n",
      "epochs:  88 loss_train:  3.2439001651283674 accuracy_train: 0.149 loss_val 3.4404595495582457 accuracy_validation: 0.129\n",
      "epochs:  89 loss_train:  3.228155793583237 accuracy_train: 0.149 loss_val 3.4248676292697393 accuracy_validation: 0.132\n",
      "epochs:  90 loss_train:  3.2207139372671842 accuracy_train: 0.148 loss_val 3.4172481508020196 accuracy_validation: 0.133\n",
      "epochs:  91 loss_train:  3.2095024982577915 accuracy_train: 0.149 loss_val 3.4066519344607387 accuracy_validation: 0.133\n",
      "epochs:  92 loss_train:  3.2037427294165632 accuracy_train: 0.149 loss_val 3.4008099710066406 accuracy_validation: 0.133\n",
      "epochs:  93 loss_train:  3.1992318276503284 accuracy_train: 0.147 loss_val 3.395638591520675 accuracy_validation: 0.132\n",
      "epochs:  94 loss_train:  3.189353609140965 accuracy_train: 0.146 loss_val 3.385511383157984 accuracy_validation: 0.133\n",
      "epochs:  95 loss_train:  3.198217082881479 accuracy_train: 0.148 loss_val 3.3922623224543877 accuracy_validation: 0.129\n",
      "epochs:  96 loss_train:  3.1687423480695784 accuracy_train: 0.145 loss_val 3.3620491948167266 accuracy_validation: 0.134\n",
      "epochs:  97 loss_train:  3.1575450399447766 accuracy_train: 0.145 loss_val 3.3511482616860575 accuracy_validation: 0.134\n",
      "epochs:  98 loss_train:  3.150441617029712 accuracy_train: 0.144 loss_val 3.34420249957144 accuracy_validation: 0.133\n",
      "epochs:  99 loss_train:  3.1388345881494404 accuracy_train: 0.146 loss_val 3.3332383975384827 accuracy_validation: 0.135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(37.06098245765916, 0.0947)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN1 = Convolutional(input_shape = (3,32,32), kernel_size = 3, output_depth = 4, l_rate = 0.01, activeFuncion = \"relu\")\n",
    "pool1 = MaxPoolingLayer(2)\n",
    "CNN2 = Convolutional(input_shape = (4,15,15), kernel_size = 3,output_depth = 8, l_rate = 0.01, activeFuncion = \"relu\")\n",
    "pool2 = MaxPoolingLayer(2)\n",
    "flatten = Flattening()\n",
    "nn = NeuralNetwork(layers_size=[392,10],activations = [\"softmax\"], lossFunction = \"crossEntropy\", l_rate = 0.01) # 8*7*7 = 392\n",
    "\n",
    "\n",
    "model = Model([CNN1,pool1,CNN2,pool2, flatten,nn])\n",
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (1000, 3, 32, 32)\n",
      "y_train:  (1000, 10)\n",
      "x_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000, 10)\n",
      "epochs:  0 loss_train:  3.132488126245623 accuracy_train: 0.149 loss_val 3.3262624279676696 accuracy_validation: 0.135\n",
      "epochs:  1 loss_train:  3.121569233424475 accuracy_train: 0.15 loss_val 3.3162861285775893 accuracy_validation: 0.136\n",
      "epochs:  2 loss_train:  3.1042927308039663 accuracy_train: 0.152 loss_val 3.3011363736284904 accuracy_validation: 0.138\n",
      "epochs:  3 loss_train:  3.100566645310213 accuracy_train: 0.15 loss_val 3.2969699678336375 accuracy_validation: 0.14\n",
      "epochs:  4 loss_train:  3.095441180941555 accuracy_train: 0.152 loss_val 3.2914728451181943 accuracy_validation: 0.139\n",
      "epochs:  5 loss_train:  3.0959322788554946 accuracy_train: 0.154 loss_val 3.29223930624152 accuracy_validation: 0.137\n",
      "epochs:  6 loss_train:  3.072456205487343 accuracy_train: 0.155 loss_val 3.269481352788643 accuracy_validation: 0.14\n",
      "epochs:  7 loss_train:  3.0644183233259614 accuracy_train: 0.154 loss_val 3.262122608086243 accuracy_validation: 0.14\n",
      "epochs:  8 loss_train:  3.064527137143143 accuracy_train: 0.157 loss_val 3.2633186848362707 accuracy_validation: 0.137\n",
      "epochs:  9 loss_train:  3.0623081212046386 accuracy_train: 0.155 loss_val 3.26123923423013 accuracy_validation: 0.138\n",
      "epochs:  10 loss_train:  3.0469235567771755 accuracy_train: 0.157 loss_val 3.2464845542577363 accuracy_validation: 0.137\n",
      "epochs:  11 loss_train:  3.039850984641179 accuracy_train: 0.154 loss_val 3.2406686560699116 accuracy_validation: 0.137\n",
      "epochs:  12 loss_train:  3.025536480442169 accuracy_train: 0.157 loss_val 3.226919354390618 accuracy_validation: 0.136\n",
      "epochs:  13 loss_train:  3.025947931216226 accuracy_train: 0.153 loss_val 3.2280552376022653 accuracy_validation: 0.134\n",
      "epochs:  14 loss_train:  3.0171920394970475 accuracy_train: 0.154 loss_val 3.2212912823064754 accuracy_validation: 0.134\n",
      "epochs:  15 loss_train:  3.00875458950306 accuracy_train: 0.16 loss_val 3.2140078576057403 accuracy_validation: 0.133\n",
      "epochs:  16 loss_train:  3.0064553972185597 accuracy_train: 0.158 loss_val 3.2129765577461202 accuracy_validation: 0.137\n",
      "epochs:  17 loss_train:  2.9842255107759432 accuracy_train: 0.162 loss_val 3.1925499664841164 accuracy_validation: 0.13\n",
      "epochs:  18 loss_train:  2.977356579556705 accuracy_train: 0.159 loss_val 3.1864462780495675 accuracy_validation: 0.131\n",
      "epochs:  19 loss_train:  2.9669563401725867 accuracy_train: 0.158 loss_val 3.17734413810929 accuracy_validation: 0.132\n",
      "epochs:  20 loss_train:  2.948358785469335 accuracy_train: 0.161 loss_val 3.1617145676208684 accuracy_validation: 0.136\n",
      "epochs:  21 loss_train:  2.937549254596686 accuracy_train: 0.163 loss_val 3.1529420816339635 accuracy_validation: 0.136\n",
      "epochs:  22 loss_train:  2.935553672720579 accuracy_train: 0.164 loss_val 3.1517624176387606 accuracy_validation: 0.138\n",
      "epochs:  23 loss_train:  2.9231522459930477 accuracy_train: 0.166 loss_val 3.141191818641837 accuracy_validation: 0.139\n",
      "epochs:  24 loss_train:  2.9172052253051515 accuracy_train: 0.165 loss_val 3.136467373708712 accuracy_validation: 0.139\n",
      "epochs:  25 loss_train:  2.9169125860921468 accuracy_train: 0.161 loss_val 3.137425516401564 accuracy_validation: 0.139\n",
      "epochs:  26 loss_train:  2.9090744823558072 accuracy_train: 0.161 loss_val 3.1316500192717736 accuracy_validation: 0.141\n",
      "epochs:  27 loss_train:  2.8922027507885297 accuracy_train: 0.167 loss_val 3.117911099171485 accuracy_validation: 0.141\n",
      "epochs:  28 loss_train:  2.8875446428935447 accuracy_train: 0.165 loss_val 3.1146349033840632 accuracy_validation: 0.14\n",
      "epochs:  29 loss_train:  2.880678884540787 accuracy_train: 0.165 loss_val 3.1096525351362123 accuracy_validation: 0.14\n",
      "epochs:  30 loss_train:  2.8727078912672552 accuracy_train: 0.166 loss_val 3.104386235332 accuracy_validation: 0.14\n",
      "epochs:  31 loss_train:  2.8641813645662335 accuracy_train: 0.166 loss_val 3.0980719311667144 accuracy_validation: 0.141\n",
      "epochs:  32 loss_train:  2.856269825779561 accuracy_train: 0.166 loss_val 3.0925517099083693 accuracy_validation: 0.141\n",
      "epochs:  33 loss_train:  2.858605379536707 accuracy_train: 0.165 loss_val 3.095866872186721 accuracy_validation: 0.139\n",
      "epochs:  34 loss_train:  2.850936099052552 accuracy_train: 0.167 loss_val 3.089910535792764 accuracy_validation: 0.14\n",
      "epochs:  35 loss_train:  2.8425651114967216 accuracy_train: 0.167 loss_val 3.082510972796639 accuracy_validation: 0.14\n",
      "epochs:  36 loss_train:  2.8410574931189023 accuracy_train: 0.165 loss_val 3.081848414670526 accuracy_validation: 0.141\n",
      "epochs:  37 loss_train:  2.832047478395594 accuracy_train: 0.164 loss_val 3.075515557563545 accuracy_validation: 0.141\n",
      "epochs:  38 loss_train:  2.8185672394454966 accuracy_train: 0.167 loss_val 3.063609470147336 accuracy_validation: 0.141\n",
      "epochs:  39 loss_train:  2.818511000587896 accuracy_train: 0.167 loss_val 3.063692010941219 accuracy_validation: 0.142\n",
      "epochs:  40 loss_train:  2.814527789007398 accuracy_train: 0.166 loss_val 3.061123811146712 accuracy_validation: 0.143\n",
      "epochs:  41 loss_train:  2.8065147133444235 accuracy_train: 0.166 loss_val 3.055283775457632 accuracy_validation: 0.142\n",
      "epochs:  42 loss_train:  2.803426727927417 accuracy_train: 0.165 loss_val 3.0551545735468064 accuracy_validation: 0.144\n",
      "epochs:  43 loss_train:  2.8015535952417654 accuracy_train: 0.163 loss_val 3.0549159700411637 accuracy_validation: 0.143\n",
      "epochs:  44 loss_train:  2.7947317056081564 accuracy_train: 0.163 loss_val 3.049268658143902 accuracy_validation: 0.143\n",
      "epochs:  45 loss_train:  2.7919160958499574 accuracy_train: 0.162 loss_val 3.0483190834778036 accuracy_validation: 0.146\n",
      "epochs:  46 loss_train:  2.7851536360785176 accuracy_train: 0.162 loss_val 3.043409205997599 accuracy_validation: 0.148\n",
      "epochs:  47 loss_train:  2.772980957413305 accuracy_train: 0.163 loss_val 3.032667627842229 accuracy_validation: 0.146\n",
      "epochs:  48 loss_train:  2.7629390114898076 accuracy_train: 0.165 loss_val 3.0248225837394673 accuracy_validation: 0.146\n",
      "epochs:  49 loss_train:  2.7581212918018205 accuracy_train: 0.164 loss_val 3.021264347567032 accuracy_validation: 0.149\n",
      "epochs:  50 loss_train:  2.7502528060144185 accuracy_train: 0.164 loss_val 3.0154424119144565 accuracy_validation: 0.149\n",
      "epochs:  51 loss_train:  2.7442123145501562 accuracy_train: 0.161 loss_val 3.009296097706776 accuracy_validation: 0.149\n",
      "epochs:  52 loss_train:  2.738950663051758 accuracy_train: 0.162 loss_val 3.00508477031442 accuracy_validation: 0.148\n",
      "epochs:  53 loss_train:  2.731981170587231 accuracy_train: 0.162 loss_val 3.0001708107978713 accuracy_validation: 0.151\n",
      "epochs:  54 loss_train:  2.7285675839659445 accuracy_train: 0.163 loss_val 2.998275402718984 accuracy_validation: 0.153\n",
      "epochs:  55 loss_train:  2.725629023166935 accuracy_train: 0.162 loss_val 2.996074520237877 accuracy_validation: 0.153\n",
      "epochs:  56 loss_train:  2.7230931150654025 accuracy_train: 0.163 loss_val 2.994552759060243 accuracy_validation: 0.156\n",
      "epochs:  57 loss_train:  2.718647967171476 accuracy_train: 0.161 loss_val 2.9910468888787727 accuracy_validation: 0.158\n",
      "epochs:  58 loss_train:  2.7140130056104166 accuracy_train: 0.161 loss_val 2.9869737464826738 accuracy_validation: 0.158\n",
      "epochs:  59 loss_train:  2.711258491470582 accuracy_train: 0.161 loss_val 2.984949710617549 accuracy_validation: 0.156\n",
      "epochs:  60 loss_train:  2.700824121794834 accuracy_train: 0.161 loss_val 2.974942692746786 accuracy_validation: 0.156\n",
      "epochs:  61 loss_train:  2.6953231428602074 accuracy_train: 0.159 loss_val 2.9703408680427827 accuracy_validation: 0.154\n",
      "epochs:  62 loss_train:  2.6920269711705878 accuracy_train: 0.162 loss_val 2.9677855831035873 accuracy_validation: 0.154\n",
      "epochs:  63 loss_train:  2.6839094207869354 accuracy_train: 0.161 loss_val 2.9615605970307906 accuracy_validation: 0.154\n",
      "epochs:  64 loss_train:  2.6765894255045755 accuracy_train: 0.161 loss_val 2.9554772117543613 accuracy_validation: 0.153\n",
      "epochs:  65 loss_train:  2.6713866030125852 accuracy_train: 0.161 loss_val 2.951017905554998 accuracy_validation: 0.154\n",
      "epochs:  66 loss_train:  2.6699441997031643 accuracy_train: 0.16 loss_val 2.9498398722656334 accuracy_validation: 0.154\n",
      "epochs:  67 loss_train:  2.664322906438328 accuracy_train: 0.16 loss_val 2.9449182939319427 accuracy_validation: 0.154\n",
      "epochs:  68 loss_train:  2.6635500350477095 accuracy_train: 0.16 loss_val 2.9449311409306818 accuracy_validation: 0.156\n",
      "epochs:  69 loss_train:  2.668268291679932 accuracy_train: 0.158 loss_val 2.949864695863235 accuracy_validation: 0.157\n",
      "epochs:  70 loss_train:  2.6623577381323673 accuracy_train: 0.158 loss_val 2.9445634044074067 accuracy_validation: 0.159\n",
      "epochs:  71 loss_train:  2.6587670438237785 accuracy_train: 0.156 loss_val 2.941254760867626 accuracy_validation: 0.159\n",
      "epochs:  72 loss_train:  2.652112960869535 accuracy_train: 0.161 loss_val 2.9352048302348104 accuracy_validation: 0.159\n",
      "epochs:  73 loss_train:  2.649325723914552 accuracy_train: 0.16 loss_val 2.9330514278895734 accuracy_validation: 0.159\n",
      "epochs:  74 loss_train:  2.645330927998935 accuracy_train: 0.16 loss_val 2.9295271376774963 accuracy_validation: 0.156\n",
      "epochs:  75 loss_train:  2.6397479792090266 accuracy_train: 0.161 loss_val 2.924639803075661 accuracy_validation: 0.155\n",
      "epochs:  76 loss_train:  2.6312855745645103 accuracy_train: 0.161 loss_val 2.91729005406796 accuracy_validation: 0.155\n",
      "epochs:  77 loss_train:  2.6230292683867535 accuracy_train: 0.164 loss_val 2.9102326028893155 accuracy_validation: 0.153\n",
      "epochs:  78 loss_train:  2.624172500897902 accuracy_train: 0.163 loss_val 2.9116916427342834 accuracy_validation: 0.154\n",
      "epochs:  79 loss_train:  2.619600691363682 accuracy_train: 0.166 loss_val 2.9078727161557847 accuracy_validation: 0.153\n",
      "epochs:  80 loss_train:  2.622423020740757 accuracy_train: 0.164 loss_val 2.909927295007567 accuracy_validation: 0.154\n",
      "epochs:  81 loss_train:  2.6188902566914094 accuracy_train: 0.164 loss_val 2.9064795359585047 accuracy_validation: 0.154\n",
      "epochs:  82 loss_train:  2.614671063146192 accuracy_train: 0.165 loss_val 2.902798551284335 accuracy_validation: 0.153\n",
      "epochs:  83 loss_train:  2.6091915304164184 accuracy_train: 0.165 loss_val 2.898109461351902 accuracy_validation: 0.153\n",
      "epochs:  84 loss_train:  2.6024012745710356 accuracy_train: 0.165 loss_val 2.89188199878986 accuracy_validation: 0.156\n",
      "epochs:  85 loss_train:  2.6048860923061006 accuracy_train: 0.165 loss_val 2.8938455225055186 accuracy_validation: 0.156\n",
      "epochs:  86 loss_train:  2.5974163851647822 accuracy_train: 0.167 loss_val 2.886490858608427 accuracy_validation: 0.158\n",
      "epochs:  87 loss_train:  2.5961355720620314 accuracy_train: 0.166 loss_val 2.8854262586562687 accuracy_validation: 0.155\n",
      "epochs:  88 loss_train:  2.5956845903531995 accuracy_train: 0.166 loss_val 2.8851134578131403 accuracy_validation: 0.155\n",
      "epochs:  89 loss_train:  2.5901415836423887 accuracy_train: 0.166 loss_val 2.880406009986848 accuracy_validation: 0.156\n",
      "epochs:  90 loss_train:  2.575446036573795 accuracy_train: 0.17 loss_val 2.8678965867145867 accuracy_validation: 0.156\n",
      "epochs:  91 loss_train:  2.5725136814636715 accuracy_train: 0.171 loss_val 2.8652014811674458 accuracy_validation: 0.155\n",
      "epochs:  92 loss_train:  2.5695229410131444 accuracy_train: 0.171 loss_val 2.8627251859856298 accuracy_validation: 0.154\n",
      "epochs:  93 loss_train:  2.563852438173836 accuracy_train: 0.171 loss_val 2.857335531450086 accuracy_validation: 0.156\n",
      "epochs:  94 loss_train:  2.557131825084702 accuracy_train: 0.172 loss_val 2.852209060774108 accuracy_validation: 0.157\n",
      "epochs:  95 loss_train:  2.5543461672329992 accuracy_train: 0.172 loss_val 2.8497626856822182 accuracy_validation: 0.156\n",
      "epochs:  96 loss_train:  2.559562323162754 accuracy_train: 0.169 loss_val 2.8551641827201455 accuracy_validation: 0.155\n",
      "epochs:  97 loss_train:  2.551326509270527 accuracy_train: 0.174 loss_val 2.848431657552802 accuracy_validation: 0.154\n",
      "epochs:  98 loss_train:  2.5536253488035703 accuracy_train: 0.169 loss_val 2.850698197905347 accuracy_validation: 0.153\n",
      "epochs:  99 loss_train:  2.5480829710337747 accuracy_train: 0.171 loss_val 2.8468583809170562 accuracy_validation: 0.154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31.69918785900125, 0.0881)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (1000, 3, 32, 32)\n",
      "y_train:  (1000, 10)\n",
      "x_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000, 10)\n",
      "epochs:  0 loss_train:  2.5444729312825465 accuracy_train: 0.171 loss_val 2.8437086080017533 accuracy_validation: 0.154\n",
      "epochs:  1 loss_train:  2.5326340505522795 accuracy_train: 0.172 loss_val 2.8335883049002923 accuracy_validation: 0.157\n",
      "epochs:  2 loss_train:  2.5363515148273232 accuracy_train: 0.173 loss_val 2.837535051642044 accuracy_validation: 0.155\n",
      "epochs:  3 loss_train:  2.5342683975599987 accuracy_train: 0.175 loss_val 2.835939970492302 accuracy_validation: 0.155\n",
      "epochs:  4 loss_train:  2.525709328629971 accuracy_train: 0.173 loss_val 2.827986039531881 accuracy_validation: 0.158\n",
      "epochs:  5 loss_train:  2.5292161378939904 accuracy_train: 0.173 loss_val 2.8316349789140864 accuracy_validation: 0.157\n",
      "epochs:  6 loss_train:  2.523312530990817 accuracy_train: 0.173 loss_val 2.8262244167810686 accuracy_validation: 0.158\n",
      "epochs:  7 loss_train:  2.5226215911159797 accuracy_train: 0.173 loss_val 2.8255967997910307 accuracy_validation: 0.157\n",
      "epochs:  8 loss_train:  2.515297020512201 accuracy_train: 0.176 loss_val 2.8193329085247014 accuracy_validation: 0.157\n",
      "epochs:  9 loss_train:  2.51401873334008 accuracy_train: 0.176 loss_val 2.8194295426095426 accuracy_validation: 0.159\n",
      "epochs:  10 loss_train:  2.510103078147829 accuracy_train: 0.178 loss_val 2.8157742980580367 accuracy_validation: 0.159\n",
      "epochs:  11 loss_train:  2.5116013738313736 accuracy_train: 0.176 loss_val 2.817387035086887 accuracy_validation: 0.161\n",
      "epochs:  12 loss_train:  2.502549025557846 accuracy_train: 0.18 loss_val 2.809550338373236 accuracy_validation: 0.162\n",
      "epochs:  13 loss_train:  2.4999327790109787 accuracy_train: 0.18 loss_val 2.807138661318421 accuracy_validation: 0.161\n",
      "epochs:  14 loss_train:  2.5006457444741064 accuracy_train: 0.18 loss_val 2.8080501151485753 accuracy_validation: 0.158\n",
      "epochs:  15 loss_train:  2.500555091388082 accuracy_train: 0.177 loss_val 2.808318902993348 accuracy_validation: 0.157\n",
      "epochs:  16 loss_train:  2.497299122721603 accuracy_train: 0.179 loss_val 2.8064549831862933 accuracy_validation: 0.159\n",
      "epochs:  17 loss_train:  2.4924984791890297 accuracy_train: 0.178 loss_val 2.80183460791179 accuracy_validation: 0.16\n",
      "epochs:  18 loss_train:  2.48761613365009 accuracy_train: 0.181 loss_val 2.797659141461074 accuracy_validation: 0.161\n",
      "epochs:  19 loss_train:  2.4860425544630718 accuracy_train: 0.181 loss_val 2.796080283102894 accuracy_validation: 0.161\n",
      "epochs:  20 loss_train:  2.4845899598314083 accuracy_train: 0.178 loss_val 2.794616297896626 accuracy_validation: 0.16\n",
      "epochs:  21 loss_train:  2.4821887215903 accuracy_train: 0.181 loss_val 2.792545634568833 accuracy_validation: 0.161\n",
      "epochs:  22 loss_train:  2.4806503381866594 accuracy_train: 0.18 loss_val 2.7907926185605696 accuracy_validation: 0.161\n",
      "epochs:  23 loss_train:  2.4783488035797876 accuracy_train: 0.18 loss_val 2.7885401893686326 accuracy_validation: 0.162\n",
      "epochs:  24 loss_train:  2.4711647842383067 accuracy_train: 0.185 loss_val 2.782202423915793 accuracy_validation: 0.162\n",
      "epochs:  25 loss_train:  2.4739624369042654 accuracy_train: 0.182 loss_val 2.784780583258747 accuracy_validation: 0.162\n",
      "epochs:  26 loss_train:  2.4680118723085624 accuracy_train: 0.183 loss_val 2.779700409277781 accuracy_validation: 0.162\n",
      "epochs:  27 loss_train:  2.465570706324472 accuracy_train: 0.185 loss_val 2.77732934393259 accuracy_validation: 0.162\n",
      "epochs:  28 loss_train:  2.463489116644568 accuracy_train: 0.184 loss_val 2.775216263198328 accuracy_validation: 0.163\n",
      "epochs:  29 loss_train:  2.4609712989667667 accuracy_train: 0.186 loss_val 2.7730638527566684 accuracy_validation: 0.163\n",
      "epochs:  30 loss_train:  2.4578878290986164 accuracy_train: 0.187 loss_val 2.7702804339383484 accuracy_validation: 0.163\n",
      "epochs:  31 loss_train:  2.4580150840731654 accuracy_train: 0.185 loss_val 2.770328624228341 accuracy_validation: 0.162\n",
      "epochs:  32 loss_train:  2.455485484436274 accuracy_train: 0.186 loss_val 2.7681423664548093 accuracy_validation: 0.165\n",
      "epochs:  33 loss_train:  2.4532994780140998 accuracy_train: 0.186 loss_val 2.7661742254620507 accuracy_validation: 0.166\n",
      "epochs:  34 loss_train:  2.4510374655950673 accuracy_train: 0.187 loss_val 2.764008164908114 accuracy_validation: 0.165\n",
      "epochs:  35 loss_train:  2.444783441158591 accuracy_train: 0.187 loss_val 2.758977758073727 accuracy_validation: 0.166\n",
      "epochs:  36 loss_train:  2.4533793747251504 accuracy_train: 0.182 loss_val 2.7659148731827994 accuracy_validation: 0.164\n",
      "epochs:  37 loss_train:  2.445611346738545 accuracy_train: 0.187 loss_val 2.758912873311113 accuracy_validation: 0.166\n",
      "epochs:  38 loss_train:  2.4471100459306925 accuracy_train: 0.184 loss_val 2.760458492945211 accuracy_validation: 0.162\n",
      "epochs:  39 loss_train:  2.444355580696253 accuracy_train: 0.185 loss_val 2.7577276361165928 accuracy_validation: 0.162\n",
      "epochs:  40 loss_train:  2.4433716163457615 accuracy_train: 0.185 loss_val 2.756706928229396 accuracy_validation: 0.163\n",
      "epochs:  41 loss_train:  2.4367991548357617 accuracy_train: 0.186 loss_val 2.750843967829724 accuracy_validation: 0.163\n",
      "epochs:  42 loss_train:  2.436466845903012 accuracy_train: 0.187 loss_val 2.7506432163892724 accuracy_validation: 0.162\n",
      "epochs:  43 loss_train:  2.4392765377756316 accuracy_train: 0.183 loss_val 2.752375791232092 accuracy_validation: 0.161\n",
      "epochs:  44 loss_train:  2.4340633102102074 accuracy_train: 0.187 loss_val 2.7481486762100866 accuracy_validation: 0.164\n",
      "epochs:  45 loss_train:  2.431447188763867 accuracy_train: 0.186 loss_val 2.745005096733635 accuracy_validation: 0.163\n",
      "epochs:  46 loss_train:  2.428658779172455 accuracy_train: 0.189 loss_val 2.7423034279749725 accuracy_validation: 0.163\n",
      "epochs:  47 loss_train:  2.4250120329940508 accuracy_train: 0.191 loss_val 2.739293027866568 accuracy_validation: 0.163\n",
      "epochs:  48 loss_train:  2.428545327578427 accuracy_train: 0.186 loss_val 2.741614232217532 accuracy_validation: 0.163\n",
      "epochs:  49 loss_train:  2.4190659678414552 accuracy_train: 0.194 loss_val 2.733101768317991 accuracy_validation: 0.164\n",
      "epochs:  50 loss_train:  2.418981245170503 accuracy_train: 0.191 loss_val 2.733497039736987 accuracy_validation: 0.164\n",
      "epochs:  51 loss_train:  2.419916540462642 accuracy_train: 0.188 loss_val 2.7333146865431672 accuracy_validation: 0.163\n",
      "epochs:  52 loss_train:  2.4183533283625525 accuracy_train: 0.19 loss_val 2.7319736415199722 accuracy_validation: 0.164\n",
      "epochs:  53 loss_train:  2.410488431210112 accuracy_train: 0.196 loss_val 2.7253614272035063 accuracy_validation: 0.165\n",
      "epochs:  54 loss_train:  2.406681532180195 accuracy_train: 0.196 loss_val 2.72168968343175 accuracy_validation: 0.168\n",
      "epochs:  55 loss_train:  2.4105894724439056 accuracy_train: 0.19 loss_val 2.725222175757094 accuracy_validation: 0.165\n",
      "epochs:  56 loss_train:  2.407624829563876 accuracy_train: 0.193 loss_val 2.7223227750730743 accuracy_validation: 0.166\n",
      "epochs:  57 loss_train:  2.40792762077303 accuracy_train: 0.189 loss_val 2.722937290731089 accuracy_validation: 0.167\n",
      "epochs:  58 loss_train:  2.4078351002269254 accuracy_train: 0.186 loss_val 2.7219121509988655 accuracy_validation: 0.167\n",
      "epochs:  59 loss_train:  2.40989036429915 accuracy_train: 0.184 loss_val 2.723682343285416 accuracy_validation: 0.167\n",
      "epochs:  60 loss_train:  2.404613349386859 accuracy_train: 0.186 loss_val 2.7185841749167143 accuracy_validation: 0.167\n",
      "epochs:  61 loss_train:  2.399101681863796 accuracy_train: 0.188 loss_val 2.7134956684760465 accuracy_validation: 0.168\n",
      "epochs:  62 loss_train:  2.3965079238783127 accuracy_train: 0.188 loss_val 2.7109631704991557 accuracy_validation: 0.168\n",
      "epochs:  63 loss_train:  2.3949890896344947 accuracy_train: 0.188 loss_val 2.7104202683146688 accuracy_validation: 0.168\n",
      "epochs:  64 loss_train:  2.3947049130970766 accuracy_train: 0.188 loss_val 2.709856581051409 accuracy_validation: 0.168\n",
      "epochs:  65 loss_train:  2.3988299750703446 accuracy_train: 0.186 loss_val 2.7130131073611308 accuracy_validation: 0.168\n",
      "epochs:  66 loss_train:  2.382153872084147 accuracy_train: 0.19 loss_val 2.6992891169018116 accuracy_validation: 0.167\n",
      "epochs:  67 loss_train:  2.389897395988768 accuracy_train: 0.189 loss_val 2.705173136013142 accuracy_validation: 0.167\n",
      "epochs:  68 loss_train:  2.3890901483782683 accuracy_train: 0.189 loss_val 2.7044899476097144 accuracy_validation: 0.168\n",
      "epochs:  69 loss_train:  2.3868500001179584 accuracy_train: 0.19 loss_val 2.702398101033516 accuracy_validation: 0.168\n",
      "epochs:  70 loss_train:  2.3884014272535143 accuracy_train: 0.186 loss_val 2.7036165653370925 accuracy_validation: 0.17\n",
      "epochs:  71 loss_train:  2.385257837660343 accuracy_train: 0.188 loss_val 2.7009661253009756 accuracy_validation: 0.17\n",
      "epochs:  72 loss_train:  2.383484567370333 accuracy_train: 0.188 loss_val 2.6992975869024303 accuracy_validation: 0.17\n",
      "epochs:  73 loss_train:  2.3755539561697896 accuracy_train: 0.192 loss_val 2.6923003234906955 accuracy_validation: 0.17\n",
      "epochs:  74 loss_train:  2.3711554928716327 accuracy_train: 0.192 loss_val 2.688539573756867 accuracy_validation: 0.17\n",
      "epochs:  75 loss_train:  2.3678103734454172 accuracy_train: 0.19 loss_val 2.6859093856406377 accuracy_validation: 0.173\n",
      "epochs:  76 loss_train:  2.3650025232824072 accuracy_train: 0.189 loss_val 2.6836883474859814 accuracy_validation: 0.173\n",
      "epochs:  77 loss_train:  2.3669404051965195 accuracy_train: 0.19 loss_val 2.6857261712282474 accuracy_validation: 0.172\n",
      "epochs:  78 loss_train:  2.355180961008958 accuracy_train: 0.189 loss_val 2.6745246380100567 accuracy_validation: 0.172\n",
      "epochs:  79 loss_train:  2.352680960782082 accuracy_train: 0.19 loss_val 2.6729477781337603 accuracy_validation: 0.172\n",
      "epochs:  80 loss_train:  2.353721419158785 accuracy_train: 0.19 loss_val 2.6738703184855805 accuracy_validation: 0.174\n",
      "epochs:  81 loss_train:  2.3429810183446302 accuracy_train: 0.193 loss_val 2.6642581450658334 accuracy_validation: 0.173\n",
      "epochs:  82 loss_train:  2.3450489881067504 accuracy_train: 0.19 loss_val 2.6674195598744803 accuracy_validation: 0.173\n",
      "epochs:  83 loss_train:  2.3406120571299875 accuracy_train: 0.194 loss_val 2.663116263783007 accuracy_validation: 0.173\n",
      "epochs:  84 loss_train:  2.337099108013696 accuracy_train: 0.193 loss_val 2.6604864157092925 accuracy_validation: 0.174\n",
      "epochs:  85 loss_train:  2.3357676422053006 accuracy_train: 0.193 loss_val 2.658774033157706 accuracy_validation: 0.173\n",
      "epochs:  86 loss_train:  2.3442714121042596 accuracy_train: 0.191 loss_val 2.6662330921838278 accuracy_validation: 0.173\n",
      "epochs:  87 loss_train:  2.3341217362322446 accuracy_train: 0.192 loss_val 2.6572117637355577 accuracy_validation: 0.172\n",
      "epochs:  88 loss_train:  2.3304174196318477 accuracy_train: 0.197 loss_val 2.653888680622676 accuracy_validation: 0.174\n",
      "epochs:  89 loss_train:  2.328854311667869 accuracy_train: 0.196 loss_val 2.6528499271306094 accuracy_validation: 0.174\n",
      "epochs:  90 loss_train:  2.3280969410836794 accuracy_train: 0.194 loss_val 2.6518344793349597 accuracy_validation: 0.173\n",
      "epochs:  91 loss_train:  2.327220978210931 accuracy_train: 0.194 loss_val 2.6514255299628857 accuracy_validation: 0.173\n",
      "epochs:  92 loss_train:  2.3249240858493514 accuracy_train: 0.195 loss_val 2.649503757133026 accuracy_validation: 0.174\n",
      "epochs:  93 loss_train:  2.325980730204198 accuracy_train: 0.195 loss_val 2.6506143994614093 accuracy_validation: 0.173\n",
      "epochs:  94 loss_train:  2.3208592492049283 accuracy_train: 0.193 loss_val 2.644764764219207 accuracy_validation: 0.18\n",
      "epochs:  95 loss_train:  2.320780695933121 accuracy_train: 0.195 loss_val 2.6446346400703726 accuracy_validation: 0.177\n",
      "epochs:  96 loss_train:  2.3173126696491297 accuracy_train: 0.194 loss_val 2.641162150556654 accuracy_validation: 0.178\n",
      "epochs:  97 loss_train:  2.3131679946062476 accuracy_train: 0.198 loss_val 2.6380234578281123 accuracy_validation: 0.179\n",
      "epochs:  98 loss_train:  2.3165640540120274 accuracy_train: 0.196 loss_val 2.640249172754693 accuracy_validation: 0.177\n",
      "epochs:  99 loss_train:  2.3096423660132896 accuracy_train: 0.196 loss_val 2.635035361241828 accuracy_validation: 0.181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(29.76801659412122, 0.0814)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 64, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dataset/test/corn/corn997.jpg has an unexpected shape: (64, 64)\n",
      "(7000, 3, 64, 64) (7000, 10)\n",
      "(1000, 3, 64, 64) (1000, 10)\n",
      "(1999, 3, 64, 64) (1999, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from MLP_autograd import *\n",
    "from CNN_autograd import *\n",
    "train_path = \"dataset/train.csv\"\n",
    "val_path = \"dataset/val.csv\"\n",
    "test_path = \"dataset/test.csv\"\n",
    "resize_to = (64, 64)\n",
    "x_train = []\n",
    "x_validation = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_validation = []\n",
    "y_test = []\n",
    "\n",
    "with open(train_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        slip_data = line.strip().split(',')\n",
    "        image_path = \"dataset/\" + slip_data[0]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(resize_to)  # Resize hnh nh\n",
    "        data = np.asarray(image)\n",
    "        x_train.append(data)\n",
    "        y_train.append(slip_data[1])\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "with open(val_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        slip_data = line.strip().split(',')\n",
    "        image_path = \"dataset/\" + slip_data[0]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(resize_to)  # Resize hnh nh\n",
    "        data = np.asarray(image)\n",
    "        x_validation.append(data)\n",
    "        y_validation.append(slip_data[1])\n",
    "x_validation = np.array(x_validation)\n",
    "y_validation = np.array(y_validation)\n",
    "\n",
    "\n",
    "\n",
    "with open(test_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        slip_data = line.strip().split(',')\n",
    "        image_path = \"dataset/\" + slip_data[0]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize(resize_to)  # Resize hnh nh\n",
    "        data = np.asarray(image)\n",
    "        if data.shape == (64, 64, 3):\n",
    "            x_test.append(data)\n",
    "            y_test.append(slip_data[1])\n",
    "        else:\n",
    "            print(f\"Image {image_path} has an unexpected shape: {data.shape}\")\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train = x_train.transpose(0, 3, 1, 2) /255.0\n",
    "x_validation = x_validation.transpose(0, 3, 1, 2) /255.0\n",
    "x_test = x_test.transpose(0, 3, 1, 2) /255.0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, np.max(Y) + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_train = one_hot(y_train)\n",
    "\n",
    "y_validation = y_validation.astype(int)\n",
    "y_validation = one_hot(y_validation)\n",
    "\n",
    "y_test = y_test.astype(int)\n",
    "y_test = one_hot(y_test)\n",
    "\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "print(x_validation.shape,y_validation.shape)\n",
    "\n",
    "print(x_test.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (7000, 3, 64, 64)\n",
      "y_train:  (7000, 10)\n",
      "x_val:  (1000, 3, 64, 64)\n",
      "y_val:  (1000, 10)\n",
      "epochs:  0 loss_train:  12.433959402167897 accuracy_train: 0.1 loss_val 12.433959402167899 accuracy_validation: 0.1\n",
      "epochs:  1 loss_train:  12.433959402167897 accuracy_train: 0.1 loss_val 12.433959402167899 accuracy_validation: 0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m nn \u001b[38;5;241m=\u001b[39m NeuralNetwork(layers_size\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2704\u001b[39m, \u001b[38;5;241m748\u001b[39m, \u001b[38;5;241m10\u001b[39m],activations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m], lossFunction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrossEntropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, l_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m) \u001b[38;5;66;03m# 8*13*13 = 392\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m Model([CNN1,pool1,CNN2,pool2, flatten,nn])\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtest(x_test,y_test)\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\CNN_autograd.py:96\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, x_train, y_train, x_val, y_val, batch_size, epochs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layers \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     92\u001b[0m   output \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mforward(output)\n\u001b[1;32m---> 96\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layers \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     99\u001b[0m   layers\u001b[38;5;241m.\u001b[39mupdate_parameter()\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:234\u001b[0m, in \u001b[0;36mTensor.softmax.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    233\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (result\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m grad) \u001b[38;5;241m/\u001b[39m grad\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m--> 234\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:39\u001b[0m, in \u001b[0;36mTensor.__add__.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m     37\u001b[0m         new_self_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m         new_self_grad \u001b[38;5;241m=\u001b[39m new_self_grad\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_self_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# print(\"bbbbbbbbbbb1\", other.data.shape, grad.shape)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m grad\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# print(\"bbbbbbbbbbb2\", other.data.shape, grad.shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:122\u001b[0m, in \u001b[0;36mTensor.dot.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    121\u001b[0m     self_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(grad, np\u001b[38;5;241m.\u001b[39mtranspose(other\u001b[38;5;241m.\u001b[39mdata))\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    124\u001b[0m     other_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata), grad)\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:163\u001b[0m, in \u001b[0;36mTensor.relu.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m(grad):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m--> 163\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:39\u001b[0m, in \u001b[0;36mTensor.__add__.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m     37\u001b[0m         new_self_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m         new_self_grad \u001b[38;5;241m=\u001b[39m new_self_grad\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_self_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# print(\"bbbbbbbbbbb1\", other.data.shape, grad.shape)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m grad\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# print(\"bbbbbbbbbbb2\", other.data.shape, grad.shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:122\u001b[0m, in \u001b[0;36mTensor.dot.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    121\u001b[0m     self_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(grad, np\u001b[38;5;241m.\u001b[39mtranspose(other\u001b[38;5;241m.\u001b[39mdata))\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    124\u001b[0m     other_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata), grad)\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:333\u001b[0m, in \u001b[0;36mTensor.flatten.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    332\u001b[0m     grad_input \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mreshape(input_shape)\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:320\u001b[0m, in \u001b[0;36mTensor.maxpooling.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    317\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[n, c, h, w] \u001b[38;5;241m==\u001b[39m max_val:\n\u001b[0;32m    318\u001b[0m                             grad_input[n, c, h, w] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m grad[n, c, i, j]\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:163\u001b[0m, in \u001b[0;36mTensor.relu.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m(grad):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m--> 163\u001b[0m       \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:39\u001b[0m, in \u001b[0;36mTensor.__add__.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m     37\u001b[0m         new_self_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m         new_self_grad \u001b[38;5;241m=\u001b[39m new_self_grad\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_self_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# print(\"bbbbbbbbbbb1\", other.data.shape, grad.shape)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m grad\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# print(\"bbbbbbbbbbb2\", other.data.shape, grad.shape)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:271\u001b[0m, in \u001b[0;36mTensor.conv.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    269\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    270\u001b[0m               grad_input[n][j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m correlate2d(grad_padded[n][i],flipped_kernel[i][j],mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    273\u001b[0m   grad_kernel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(other\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:346\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, backward_grad)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m backward_grad\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepends_on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_grad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dotie\\My tnh\\Multi-layer-Perceptron\\autograd.py:314\u001b[0m, in \u001b[0;36mTensor.maxpooling.<locals>._backward\u001b[1;34m(grad)\u001b[0m\n\u001b[0;32m    312\u001b[0m w_start \u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m*\u001b[39m pool_size\n\u001b[0;32m    313\u001b[0m w_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(w_start \u001b[38;5;241m+\u001b[39m pool_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m--> 314\u001b[0m max_val \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mh_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mw_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(h_start, h_end):\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(w_start, w_end):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CNN1 = Convolutional(input_shape = (3,64,64), kernel_size = 5, output_depth = 8, l_rate = 0.01, activeFuncion = \"relu\")\n",
    "pool1 = MaxPoolingLayer(2)\n",
    "CNN2 = Convolutional(input_shape = (8,30,30), kernel_size = 5,output_depth = 16, l_rate = 0.01, activeFuncion = \"relu\")\n",
    "pool2 = MaxPoolingLayer(2)\n",
    "flatten = Flattening()\n",
    "nn = NeuralNetwork(layers_size=[2704, 748, 10],activations = [\"relu\", \"softmax\"], lossFunction = \"crossEntropy\", l_rate = 0.01) # 8*13*13 = 392\n",
    "\n",
    "\n",
    "model = Model([CNN1,pool1,CNN2,pool2, flatten,nn])\n",
    "model.train(x_train,y_train,x_validation,y_validation, batch_size = 128, epochs = 100)\n",
    "model.test(x_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
